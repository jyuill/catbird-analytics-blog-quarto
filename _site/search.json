[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Catbird Analytics",
    "section": "",
    "text": "GGplot Theme Sampler: Choosing a Theme\n\n\n\n\n\n\ndataviz\n\n\nr\n\n\n\n\n\n\n\n\n\nOct 20, 2024\n\n\nJohn Yuill\n\n\n\n\n\n\n\n\n\n\n\n\nBC Beer Sales Analysis: Commercial, Regional, Micro Brew 2016-2023\n\n\n\n\n\n\nliquor stats\n\n\nreporting\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 30, 2024\n\n\nJohn Yuill\n\n\n\n\n\n\n\n\n\n\n\n\nBC Liquor Sales Analysis: Beer 2016-2023\n\n\n\n\n\n\nliquor stats\n\n\nbeer\n\n\nreporting\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\nJohn Yuill\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation is NOT Causation…It May Not Even Be Correlation\n\n\n\n\n\n\nanalysis\n\n\nr\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nMar 24, 2024\n\n\nJohn Yuill\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Shinylive Experiment: Promising for Limited Use\n\n\n\n\n\n\nR\n\n\nquarto\n\n\nshiny\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nJohn Yuill\n\n\n\n\n\n\n\n\n\n\n\n\nBC Liquor Sales Analysis: Quarter-of-Year Patterns 2015-2023\n\n\n\n\n\n\nliquor stats\n\n\nreporting\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nJohn Yuill\n\n\n\n\n\n\n\n\n\n\n\n\nBC Liquor Sales Analysis: Annual Trends 2016-2022\n\n\n\n\n\n\nliquor stats\n\n\nreporting\n\n\nanalysis\n\n\nR\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nJohn Yuill\n\n\n\n\n\n\n\n\n\n\n\n\nCrypto Currency (BTC) PRices: Any Tradeable Daily Patterns?\n\n\n\n\n\n\nR\n\n\nanalysis\n\n\ncrpto-currency\n\n\n\n\n\n\n\n\n\nMay 28, 2022\n\n\nJohn Yuill\n\n\n\n\n\n\n\n\n\n\n\n\nR Time Series Objects vs Data Frames: Advantages and Limitations\n\n\n\n\n\n\nR\n\n\ntime-series\n\n\n\n\n\n\n\n\n\nJan 30, 2022\n\n\nJohn Yuill\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Trends + R: gtrendsR for Powerful Trend Analytics\n\n\n\n\n\n\nR\n\n\nAPIs\n\n\n\n\n\n\n\n\n\nJan 16, 2022\n\n\nJohn Yuill\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/correlation-check-assumptions/index.html",
    "href": "posts/correlation-check-assumptions/index.html",
    "title": "Correlation is NOT Causation…It May Not Even Be Correlation",
    "section": "",
    "text": "Most people recognize that correlation does not imply causation, and that if correlation between two variables is detected, further steps must be taken to determine if there is indeed a causal relationship.\nBut before we even get to pinning down causation, there are important steps to ensure that the correlation is valid. Correlation calculations are based on specific assumptions that, if not met, render the outcome of these calculations less than reliable. Proceeding without checking these assumptions can lead to erroneous conclusions and bad decisions.\nSo let’s talk about how we can dig into these assumptions and check how solid the ground is that we are standing on when referencing a correlation coefficient."
  },
  {
    "objectID": "posts/correlation-check-assumptions/index.html#intro",
    "href": "posts/correlation-check-assumptions/index.html#intro",
    "title": "Correlation is NOT Causation…It May Not Even Be Correlation",
    "section": "",
    "text": "Most people recognize that correlation does not imply causation, and that if correlation between two variables is detected, further steps must be taken to determine if there is indeed a causal relationship.\nBut before we even get to pinning down causation, there are important steps to ensure that the correlation is valid. Correlation calculations are based on specific assumptions that, if not met, render the outcome of these calculations less than reliable. Proceeding without checking these assumptions can lead to erroneous conclusions and bad decisions.\nSo let’s talk about how we can dig into these assumptions and check how solid the ground is that we are standing on when referencing a correlation coefficient."
  },
  {
    "objectID": "posts/correlation-check-assumptions/index.html#the-point-of-correlation",
    "href": "posts/correlation-check-assumptions/index.html#the-point-of-correlation",
    "title": "Correlation is NOT Causation…It May Not Even Be Correlation",
    "section": "The Point of Correlation",
    "text": "The Point of Correlation\nA correlation coefficient is intended to summarize the presence, strength and direction of a relationship between two variables. This has a variety of applications, including predictive analysis. While you can’t predict the value of one variable from another based on a correlation coefficient alone - you need linear regression (or related method) for that - you can form an expectation of how one variable will change (or not) based on changes in the other.\nFor example, if you have a strong correlation between variables (say greater than 0.5) you can:\n\nmake decisions based on expectation that as one variable rises, the other will as well, even in the absence of direct causality.\ntake actions to influence one variable in hopes of influencing the other, possibly as a test of causality.\nuse this information to include or exclude a variable in a statistical model, such as linear regression, causal impact, and more.\n\nIf the correlation coefficient is not a reliable summary of the relationship, this can lead to incorrect assumptions, wasted time and effort, and unexpected negative consequences due to decision-making based on faulty information.\nSo this is why it is important to understand when we can rely on a correlation calculation, and when we can’t."
  },
  {
    "objectID": "posts/correlation-check-assumptions/index.html#the-six-assumptions",
    "href": "posts/correlation-check-assumptions/index.html#the-six-assumptions",
    "title": "Correlation is NOT Causation…It May Not Even Be Correlation",
    "section": "The Six Assumptions",
    "text": "The Six Assumptions\nThere are different ways to calculate correlation, but by far the most common is the Pearson Correlation Coefficient for linear correlation. This is the formula that results in the famous ‘r’ value, ranging from 1 to -1, where 1 is perfect positive correlation (same direction), -1 is perfect negative correlation (opposite direction) and 0 is no correlation. Importantly, the accuracy of this calculation depends on several key assumptions, nicely described by chatGPT:\n“For the Pearson correlation coefficient to provide meaningful and reliable results, several key assumptions must be met:\n\nLinearity: The relationship between the two variables is linear, meaning the best-fit line through the data points is a straight line. This implies that as one variable increases or decreases, the other variable does so in a consistent manner.\nHomoscedasticity: The variances along the line of best fit remain similar as we move along the line. This means that the spread (variance) of data points around the line of best fit is roughly constant across all levels of the independent variable.\nNormality: The Pearson correlation assumes that both variables are normally distributed. More specifically, the assumption is about the normality of the distribution of values for each variable, and ideally, the joint normality of the two variables. However, in practice, it’s the distribution of the variables themselves that’s most scrutinized.\nInterval or Ratio Scale: Both variables should be measured on either an interval or ratio scale. This means that the variables should represent numeric values where meaningful amounts of differences between measurements can be determined.\nAbsence of Outliers: Outliers can have a disproportionate effect on the correlation coefficient, pulling the line of best fit and therefore the correlation measure in their direction. It’s important that there are no outliers in the data, or that their impact is minimized, perhaps through robust statistical techniques.\nIndependence of Observations: Each pair of observations is independent of each other pair. This means the data points collected do not influence each other, which is particularly relevant in time-series data where this assumption might be violated due to autocorrelation.\n\nWhen these assumptions are not met, the Pearson correlation coefficient might not be the appropriate measure of association between two variables.”"
  },
  {
    "objectID": "posts/correlation-check-assumptions/index.html#look-at-the-data",
    "href": "posts/correlation-check-assumptions/index.html#look-at-the-data",
    "title": "Correlation is NOT Causation…It May Not Even Be Correlation",
    "section": "Look at the Data",
    "text": "Look at the Data\nFirst and foremost: look at the data. This is the beautifully illustrated with Anscombe’s quartet, where each data set has the same Pearson’s correlation value (among other summary metrics).\n\n\nCode\n## put functions here\nans_cor &lt;- function(data, x, y, anum) {\n  xe &lt;- enquo(x) # convert var reference\n  ye &lt;- enquo(y) # convert var reference\n  xes &lt;- as.character(xe)[2] # convert to string for use in annotation\n  yes &lt;- as.character(ye)[2] \n  x_lab &lt;- max(data[[xes]])*0.75 # set x location for annotation\n  y_lab &lt;- max(data[[yes]]*1.05) # set y location for annotation\n  \n  plot &lt;- data %&gt;% ggplot(aes(x=!!xe, y=!!ye))+geom_point()+\n    geom_smooth(method='lm', se=FALSE)+\n    geom_smooth(method='loess', se=FALSE, color='green', linetype='dashed', size=0.8)+\n    stat_cor()+\n    annotate(geom='text',y=y_lab, x=x_lab, label=anum, size=8, color='red')\n  plot\n}\n# run functions to get info\na1 &lt;- ans_cor(data=ans, x=x1, y=y1, anum='I')\na2 &lt;- ans_cor(data=ans, x=x2, y=y2, anum='II')\na3 &lt;- ans_cor(data=ans, x=x3, y=y3, anum='III')\na4 &lt;- ans_cor(data=ans, x=x4, y=y4, anum='IV')\n\n\nHere are the Anscombe quartet charts - blue line is linear regression, green line is a loess smoothed line as reference. Ideally, it should track close to the blue line if linearity is strong."
  },
  {
    "objectID": "posts/correlation-check-assumptions/index.html#checking-assumptions",
    "href": "posts/correlation-check-assumptions/index.html#checking-assumptions",
    "title": "Correlation is NOT Causation…It May Not Even Be Correlation",
    "section": "Checking Assumptions",
    "text": "Checking Assumptions\nSo clearly some reason for skepticism about whether the assumptions hold up in most of these cases. Let’s take a closer look, one at a time:\n\nLinearity of relationship\n\ncriteria: relationship between the two variables is linear, best-fit line through the data points is a straight line, as one variable increases or decreases, other variable does so in a consistent manner.\nmethod: visual inspection via scatterplot, preferably with linear regression line.\nconclusion:\n\nI: somewhat.\nII: no; at lower x values but then inverts.\nIII: yes - except outlier.\nIV: no.\n\n\nSo right away that knocks out cases II and IV:\n\nPearson correlation coefficient between variables is NOT going to be a useful, reliable statistic for understanding the relationship between the variables involved.\n\nLet’s continue on with testing further assumptions focusing on cases I and III (with cases II and IV for additional reference).\n\n\nHomoscedasticity\n\ncriteria: variances along the line of best fit remain similar as we move along the line; spread (variance) of data points around the line of best fit is roughly constant.\nmethod: residuals plot, Breusch-Pagan test\n\n\n\nCode\nhomoScedTest &lt;- function(data, x, y, anum) {\n  xe &lt;- enquo(x) # convert var reference\n  ye &lt;- enquo(y) # convert var reference\n  xes &lt;- as.character(xe)[2] # convert to string for use in annotation\n  yes &lt;- as.character(ye)[2] \n  x_lab &lt;- max(data[[xes]])*0.75 # set x location for annotation\n  y_lab &lt;- max(data[[yes]]) # set y location for annotation\n  \n  formula &lt;- paste0(quo_name(ye), \" ~ \", quo_name(xe))\n  # calculate model\n  model &lt;- lm(formula, data=data)\n  # add residuals and fitted values to the data for plotting\n  data$residuals &lt;- residuals(model)\n  data$fitted &lt;- fitted(model)\n  x_lab &lt;- min(data$fitted)+0.5\n  y_lab &lt;- max(data$residuals)\n  ## generate plot\n  plot &lt;- data %&gt;% ggplot(aes(x=fitted, y=residuals))+geom_point()+\n      geom_hline(yintercept=0, linetype='dashed', color='red')+\n      geom_segment(aes(xend=fitted, yend=0), alpha=0.5)+\n      annotate(geom='text',y=y_lab, x=x_lab, label=anum, size=8, color='red')+\n      labs(x='Fitted Values', y='Residuals', title='Residual Plot')+\n      theme_minimal()\n  # Breusch-Pagan test for homoscedasticity\n  bpt_results &lt;- bptest(model)\n  # put results in table\n  bpt_row &lt;- data.frame(case=anum, BP_statistic=bpt_results$statistic, p_value=bpt_results$p.value)\n  row.names(bpt_row) &lt;- anum\n  bpt_row &lt;- bpt_row %&gt;% mutate(\n    conclusion=ifelse(p_value&gt;0.05,\"Homoscedasticity: null hypothesis holds.\",\"Homoscedasticity issues: null hypotheis fails.\")\n  )\n  # return results from plot and BP test\n  list(plot=plot, bpt=bpt_results,bpt_row=bpt_row)\n}\n#anss &lt;- homoScedTest(ans, x1, y1, 'I')\n#anss$plot\n#anss$bpt\n\n\n\n\nCode\nhskt_01 &lt;- homoScedTest(data=ans, x=x1, y=y1, anum='I')\nhskt_02 &lt;- homoScedTest(data=ans, x=x2, y=y2, anum='II')\nhskt_03 &lt;- homoScedTest(data=ans, x=x3, y=y3, anum='III')\nhskt_04 &lt;- homoScedTest(data=ans, x=x4, y=y4, anum='IV')\n\n\n\nResiduals Plots\nLet’s take a look at residuals plotted over the fitted values y volumes. In other words, using a simple linear regression model to predict the y-values based on x-values, what are the differences between the predicted values vs actual values provided? These are the residuals. In context of homoscedasticity, is there a balanced pattern in the residuals, OR are they changing (in particular growing) as we move to higher values of prediction?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNone of these look very good from a residuals point of view. Case I is the closest although residuals seem to be increasing as we move along the y-axis. This is especially important if we want to understand if the correlation will hold over larger values.\n\n\nBreusch-Pagan Test\nBreusch-Pagan is a statistical test that can be used to assess homoscedasticity. The NULL hypothesis is that the residuals have homoscedasticity (consistency in residuals), and the test evaluates how well the dataset supports this hypothesis. A p-value is used to estimate the likelihood that the current dataset is consistent with the NULL hypothesis. Low p-value (&lt;0.05) indicates that the NULL hypothesis is not supported and the data is likely NOT homoscedastic and the assumption required for Pearson Correlation does not hold.\n\n\nCode\nbptr_table &lt;- bind_rows(hskt_01$bpt_row, hskt_02$bpt_row, \n                        hskt_03$bpt_row, hskt_04$bpt_row)\nbptr_table[] &lt;- lapply(bptr_table, function(x)\n  if(is.numeric(x)) round(x,2) else (x))\ngt(bptr_table)\n\n\n\n\n\n\n\n\ncase\nBP_statistic\np_value\nconclusion\n\n\n\n\nI\n0.66\n0.42\nHomoscedasticity: null hypothesis holds.\n\n\nII\n0.00\n1.00\nHomoscedasticity: null hypothesis holds.\n\n\nIII\n2.72\n0.10\nHomoscedasticity: null hypothesis holds.\n\n\nIV\n1.18\n0.28\nHomoscedasticity: null hypothesis holds.\n\n\n\n\n\n\n\nResults here are interesting: NULL hypothesis of homoscedasticity is upheld, to varying degrees for each case, including case 2 where p-value = 1.\n\nHomoscedasticity conclusion:\n\nBased on residuals plots, there are clearly issues with homoscedasticity to varying degrees with all the datasets.\nVisual inspection of residuals charts in these cases seems much more reliable than the Breusch-Pagan test.\nSmall sample size affects reliability of Breusch-Pagan test: at least 30 observations recommended for reliable BP test, and at least 50 is even better. Here we are dealing with only 11 observations.\n\n\nThe homoscedasticity testing gives more reasons to be skeptical about the value of Pearson correlation for this data.\n\n\n\nNormality\n\ncriteria: Pearson correlation assumes that each variable is normally distributed.\nmethodology: QQ plot, Shapiro-Wilk test\n\n\n\nCode\nnormalTest &lt;- function(data, var, vars) {\n  # specify variables covered - for chart title\n  vars &lt;- paste0(vars, collapse=', ')\n  # convert variable name\n  vare &lt;- enquo(var)\n  # convert to string for use with shapiro-wilk\n  vare_char &lt;- as.character(vare)[[2]]\n  # create plot\n  plot &lt;- data %&gt;% ggplot(aes(sample=!!vare))+stat_qq()+stat_qq_line()+\n  ggtitle(paste0(\"Variable \",vars,\": Q-Q Plot\"))+\n  theme_minimal()\n  # check shapiro-wilk test\n  sw_test &lt;- shapiro.test(data[[vare_char]])\n  sw_test_row &lt;- data.frame(dataset=vars,\n                            method=sw_test$method,\n                            w=sw_test$statistic,\n                            p_value=sw_test$p.value)\n  row.names(sw_test_row) &lt;- vare_char\n  # return plot and sw test results\n  list(plot=plot, sw_test=sw_test, sw_test_row=sw_test_row)\n}\n\n\n\n\nCode\n# first 3 vars are same\nnorm_01 &lt;- normalTest(data=ans, var=x1, vars=c('x1','x2','x3'))\nnorm_04 &lt;- normalTest(data=ans, var=x4, vars=c('x4'))\nnorm_01a &lt;- normalTest(data=ans, var=y1, vars=c('y1'))\nnorm_02a &lt;- normalTest(data=ans, var=y2, vars='y2')\nnorm_03a &lt;- normalTest(data=ans, var=y3, vars='y3')\nnorm_04a &lt;- normalTest(data=ans, var=y4, vars='y4')\n\n\n\nQQ Plots\nQQ (Quantile-Quantile) Plots show how well a given variable fits with a Normal distribution (can be used with other distributions, but we’re focused on Normal). The key thing to know for interpretation is that if the data follows a Normal distribution, the points will approximately lie along the line shown in the chart. Large or consistent deviations from the line suggest deviations from the theoretical distribution, meaning that the Normality assumption required for Pearson correlation is not being met.\nThere are a total of 8 variables in the dataset, but the first three (x1, x2, x3) have the same values, so there is just one Q-Q plot for these variables. So six plots cover all the variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom these six plots we see:\n\nvariables x1/x2/x3 & y1: looks like a fit with Normality for the most part.\nx4, y2: no\ny3, y4: yes, BUT with outliers that are cause for concern.\n\n\n\nShapiro-Wilk Test\nSo we have some initial conclusions from Q-Q plots, which provide a quick visual guide. Let’s compare to results from Shapiro-Wilkes test.\nShapiro-Wilkes works similar to to Breusch-Pagan test, but with reference to Normal distribution:\n\nNULL hypothesis is Normality, p-value indicates likelihood of the dataset being consistent with the Normality null hypothesis.\nLow p-value indicates lack of support for the NULL hypothesis and suggests data set may not fit Normal distribution.\n\n\n\nCode\n# create table of results from all shapiro-wilk tests\nshap_wilk_table &lt;- bind_rows(norm_01$sw_test_row,\n                       norm_04$sw_test_row,\n                       norm_01a$sw_test_row,\n                       norm_02a$sw_test_row,\n                       norm_03a$sw_test_row,\n                       norm_04a$sw_test_row)\nshap_wilk_table &lt;- shap_wilk_table %&gt;% mutate(\n    conclusion=ifelse(p_value&gt;0.05,\"Norm. dist.: null hypothesis holds.\",\"Norm. dist. issues: null hypotheis fails.\")\n  )\nshap_wilk_table[] &lt;- lapply(shap_wilk_table, function(x)\n  if(is.numeric(x)) round(x,2) else (x))\nshap_wilk_table %&gt;% select(-method) %&gt;% gt()\n\n\n\n\n\n\n\n\ndataset\nw\np_value\nconclusion\n\n\n\n\nx1, x2, x3\n0.97\n0.87\nNorm. dist.: null hypothesis holds.\n\n\nx4\n0.34\n0.00\nNorm. dist. issues: null hypotheis fails.\n\n\ny1\n0.98\n0.95\nNorm. dist.: null hypothesis holds.\n\n\ny2\n0.83\n0.02\nNorm. dist. issues: null hypotheis fails.\n\n\ny3\n0.83\n0.03\nNorm. dist. issues: null hypotheis fails.\n\n\ny4\n0.88\n0.09\nNorm. dist.: null hypothesis holds.\n\n\n\n\n\n\n\nBased on Shapiro-Wilk test, we have a mix of Normal and non-Normal distributions.\n\nNormality conclusions\n\nOverall similar results between Q-Q plots and Shapiro-Wilk.\nx1/x2/x3/y1: look close to Normal in Q-Q plots and also pass Shapiro-Wilk test.\nx4/y2: doesn’t fit with Normal on Q-Q plot or Shapiro-Wilk test.\ny3: borderline based on Q-Q plot due to outliers, fails Shapiro-Wilk test.\ny4: borderline based on Q-Q plot due to outliers, passes Shapiro-Wilk test.\n\n\nSo as far as the Normal distribution assumption for Pearson correlation, it depends on the combination of variables being used from this data set.\n\n\n\nInterval or Ratio Scale\n\ncriteria: Pearson correlation only works on numeric values.\nmethodology: check class of variables.\n\n\n\nCode\nstr(ans)\n\n\n'data.frame':   11 obs. of  8 variables:\n $ x1: num  10 8 13 9 11 14 6 4 12 7 ...\n $ x2: num  10 8 13 9 11 14 6 4 12 7 ...\n $ x3: num  10 8 13 9 11 14 6 4 12 7 ...\n $ x4: num  8 8 8 8 8 8 8 19 8 8 ...\n $ y1: num  8.04 6.95 7.58 8.81 8.33 ...\n $ y2: num  9.14 8.14 8.74 8.77 9.26 8.1 6.13 3.1 9.13 7.26 ...\n $ y3: num  7.46 6.77 12.74 7.11 7.81 ...\n $ y4: num  6.58 5.76 7.71 8.84 8.47 7.04 5.25 12.5 5.56 7.91 ...\n\n\n\nconclusion: all variables are numeric, so we are on solid ground here.\n\n\n\nAbsence of Outliers\n\ncriteria: ideally, no outliers, as they can have a disproportionate affect on the correlation coefficient.\nmethod: boxplot for visual inspection.\n\n\nBoxplot\nA boxplot can be simple way to see if outliers exist in the variables. The interpretation is:\n\nmid-line is median.\nbox covers the IQR (inter-quartlie range) from 25th percentile to 75th percentile, covering the middle 50% of the data.\nwhiskers reach out to values that are 1.5 below/above the 25th and 75th percentiles, respectively.\nDOTS represent outliers: data points beyond the whiskers represent outliers - these are what we are watching for.\n\n\n\nCode\n#ans %&gt;% ggplot(aes(x=x1))+geom_boxplot()\nans_long &lt;- ans %&gt;% pivot_longer(cols=everything(), names_to=\"variable\", values_to = \"value\")\nans_long %&gt;% ggplot(aes(x=value, y=variable))+geom_boxplot(fill='grey')+\n  coord_flip()\n\n\n\n\n\n\n\n\n\nThe variables with dots are sure signs of outliers.\n\nOutlier conclusions:\n\nx4, y2, y3, y4 all have outliers: use caution with Pearson correlation involving these metrics.\n\n\n\n\n\nIndependence of Observations\n\ncriteria: each pair of observations in the data set is independent of each other, do not influence each other.\nmethod: knowledge of the origin and meaning of the data.\n\nDomain knowledge is needed for this one. We can’t make any conclusions on the Anscombe dataset because we don’t know anything about the origin of the data or what it represents.\nnotes:\n\ncheck assumptions\nstate if they are violated\nwhat to do if not met? other options"
  },
  {
    "objectID": "posts/correlation-check-assumptions/index.html#is-correlation-valid-for-this-data-set",
    "href": "posts/correlation-check-assumptions/index.html#is-correlation-valid-for-this-data-set",
    "title": "Correlation is NOT Causation…It May Not Even Be Correlation",
    "section": "Is Correlation Valid for this Data Set?",
    "text": "Is Correlation Valid for this Data Set?\nWe know that Anscombe’s Quartet is designed to show how summary statistics can be misleading. But for the sake of completion here, let’s review whether quartet of variable combinations ‘pass’ or ’fail’ against the 6 assumptions:\n\nLinearity: pass for case I (var x1 and y1), case III (x3 and y3); fail for case II and IV.\nHomoscedasticity: leans toward fail for all, although hard to make solid conclusion on small sample size.\nNormality: pass for case I; fail for others, where at least one variable fails normality test\nInterval or Ratio Scale: pass for all.\nAbsence of Outliers: pass for case I; fail for others, where at least one variable has outliers.\nIndependence of Observations: inconclusive - requires domain knowledge.\n\nIn summary, case I looks like a reasonable case for Pearson Correlation, while others have one or more fatal flaws that make Pearson Correlation misleading. This is fairly obvious on initial view of the data, but further reinforced by examining the assumptions individually."
  },
  {
    "objectID": "posts/correlation-check-assumptions/index.html#conclusion",
    "href": "posts/correlation-check-assumptions/index.html#conclusion",
    "title": "Correlation is NOT Causation…It May Not Even Be Correlation",
    "section": "Conclusion",
    "text": "Conclusion\nThis is a manufactured case that may not reflect common real-world data situations, but is hopefully useful in understanding how to think critically about Pearson correlation - including using visual and statistical methods to determine how applicable/reliable/representative Pearson correlation may be in any given situation.\nThis highlights the importance of understanding correlation as a summary statistic that can only tell you so much about the relationships in the data - and in some cases, nothing at all. At the end of the day, if the correlation coefficient can’t tell you something about what to expect from the dependent variable when the independent variable changes, it is not helpful. As such, it is best used within early stages of exploratory analysis, supplemented by visualization (at minimum), pointing the way toward potential further analysis, and not as a final conclusion.\n\nBonus: What to do when Pearson Correlation fails?\nIf visualization and/or addition assumption checks suggest that Pearson Correlation is not a good statistic for your dataset, what to do? It depends on the characteristics of the data, and is a more involved conversation for another day, but here are some thought-starters:\n\nData transformation, such as log: transforming the data can be a way to bring it more into line with the required assumptions of Pearson Correlation (or other technique being used). Log, Square Root, Inverse, Box-Cox are among potential transformations to try, although they tend to make interpretation more complex.\nSpearman’s Rank Correlation: doesn’t assume linear relationship; can be useful when both variables tend to increase, but not necessarily constant rate.\nKendall’s Tau: similar to Spearman’s but more robust for small sample sizes.\nMutual information: measures amount of information obtained about one variable through the other; applies beyond linear relationships.\nGeneralized Additive Models (GAMs): allow for flexibility in specifying the form of relationship between variables, accomodating complex patterns.\nQuantile Regression: can be relevant for exploring how variables vary across different points of the distribution of the dependent variable.\nRegression Trees / Random Forests: can be effective in identifying complex interactions and non-linear relationships, handling multiple predictors and their interactions.\ndata transformation:\n\nOf course, each of these come with their own set of assumptions and conditions to take into account before making a selection."
  },
  {
    "objectID": "posts/daily-patterns-in-crypto-currency-prices/index.html",
    "href": "posts/daily-patterns-in-crypto-currency-prices/index.html",
    "title": "Crypto Currency (BTC) PRices: Any Tradeable Daily Patterns?",
    "section": "",
    "text": "Intro\nI’ve come across discussion on day-of-week patterns in Bitcoin in various circles, with the most common suggestion being that Bitcoin tends to be higher during the weekdays than on weekend days. Just because somebody says so, doesn’t make it so today, even if it may have been true in the past.\nSo do any patterns really exist? And, more importantly: if they do exist, are there profitable trades to be made on a reliable basis?\n\n\ntl;dr\nSurprise…No. At least as far as I can tell. ;)\nRead on for the details! Of course, there may be flaws, errors, or ommissions in my analysis. Use at your own risk - this is merely for infotainment purposes and is not at all intended as any sort of financial or trading advice. :)\n\n\nGet Data\n\nFocus on recent years, due to long-term volatility and evolution of the market focus.\nLooking at Bitcoin as the apex crypto currency. Other coins may have entirely different patterns.\nUsing BTC-CAD because…well, I’m Canadian and I trade in Canadian $.\n\n\n## get data\nsymb &lt;- c('BTC-CAD')\ndate_st &lt;- '2020-05-01'\ndate_end &lt;- '2023-04-22'\n## using auto.assign = FALSE and setting object name to avoid issues with default 'BTC-CAD' name\nBTC_CAD &lt;- getSymbols(Symbols=symb, from=date_st, to=date_end, auto.assign = FALSE)\n\n\n\nInitial Look at Data\n\ndygraph(BTC_CAD[,\"BTC-CAD.Close\"])\n\n\n\n\n\n\nFocus on Recent Trading Range\n\nrecent history is likely to be more representative of future results (?)\nsomewhat arbitrary - picking a point that seems to represent current ranges\n\n\nrec &lt;- '2022-06-18/'\nbtc_rec &lt;- BTC_CAD[rec]\ndygraph(btc_rec[,\"BTC-CAD.Close\"])\n\n\n\n\n\n\n\n\nAdd Days of Week\n\nAdd weekdays to identify and compare prices by day of week.\nAdd date of each week to identify and compare weeks.\n\nEasiest - for me, at least - to convert time series to data frame:\n\nbtc_rec_df &lt;- data.frame(btc_rec)\nbtc_rec_df$date &lt;- index(btc_rec)\n## add days\nbtc_rec_df$day &lt;- weekdays(index(btc_rec), abbreviate = TRUE)\n## - set days to factors\nbtc_rec_df$day &lt;- factor(btc_rec_df$day)\nbtc_rec_df$day &lt;- fct_relevel(btc_rec_df$day, c(\"Sun\",\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\"))\n## add weeks\nbtc_rec_df$week_of &lt;- floor_date(btc_rec_df$date, unit='weeks')\n\nView the structure of the data with additional components added:\n\n\n'data.frame':   309 obs. of  9 variables:\n $ BTC.CAD.Open    : num  26671 24766 26725 26737 26796 ...\n $ BTC.CAD.High    : num  27013 26928 27167 27933 26976 ...\n $ BTC.CAD.Low     : num  23069 23536 25645 26429 25699 ...\n $ BTC.CAD.Close   : num  24774 26725 26743 26785 25910 ...\n $ BTC.CAD.Volume  : num  54725903315 45938111198 40010094692 37466596738 37043219273 ...\n $ BTC.CAD.Adjusted: num  24774 26725 26743 26785 25910 ...\n $ date            : Date, format: \"2022-06-18\" \"2022-06-19\" ...\n $ day             : Factor w/ 7 levels \"Sun\",\"Mon\",\"Tue\",..: 7 1 2 3 4 5 6 7 1 2 ...\n $ week_of         : Date, format: \"2022-06-12\" \"2022-06-19\" ...\n\n\n\n\nComparative views\nTake a look at daily comparisons for this period:\n\nbtc_rec_df %&gt;% ggplot(aes(x=day, y=BTC.CAD.Close))+geom_boxplot(fill=fill_color)+\n  scale_y_continuous(labels=dollar_format())+\n  labs(title=\"Distrib. of BTC-CAD Closing Price by Day\", y='BTC-CAD Close', x=\"\")\n\n\n\n\n\n\n\n\nNo strong, obvious pattern over the period.\n\nMaybe Tue-Sat?\n\nThis data is over a period where there is a lot of variance in the price that may blur the daily patterns. Could still be that there is a consistent pattern within weeks.\nFor more granularity, let’s look at week-by-week trends by day:\n\nweek_plot &lt;- btc_rec_df %&gt;% ggplot(aes(x=day, y=BTC.CAD.Close, color=as.factor(week_of), group=week_of))+geom_line()+\n  scale_y_continuous(labels=dollar_format())+\n  scale_x_discrete(expand=c(0,0))+\n  labs(title=\"BTC-CAD Closing Price by Day by Week\",x=\"\",y=\"BTC-CAD Close\")+\n  theme(legend.position = 'none')\nggplotly(week_plot)\n\n\n\n\n\nSome weeks with upward trend through the week, others declining or flat: not exactly a consistent pattern to rely on across this date range.\n\n\nSpecific Day of Weeks Comparisons\nLook at some individual day of week comparisons\n\nTue - Sat\n\nSlope chart\n\nday_01 &lt;- 'Tue'\nday_02 &lt;- 'Sat'\nbtc_rec_d_df &lt;- btc_rec_df %&gt;% filter(day==day_01 | day==day_02)\ndd_plot &lt;- btc_rec_d_df %&gt;% ggplot(aes(x=day, y=BTC.CAD.Close, color=as.factor(week_of), group=week_of))+geom_line()+\n  scale_y_continuous(labels=dollar_format())+\n  scale_x_discrete(expand=c(0,0))+\n  labs(title=\"BTC-CAD Price by Day\",x=\"\",y=\"BTC-CAD Close\")+\n  theme(legend.position = 'none')\nggplotly(dd_plot)\n\n\n\n\n\nPretty hard to pick out any obvious/consistent pattern. Let’s take a closer look:\n\nget each week % change for these days.\nget summary stats on these changes.\nlook at distribution of % change to see if any consistency.\n\n\n\nHistogram\n\n## may need to remove first row if starts with the day later in the week\nbtc_rec_d_df &lt;- btc_rec_d_df[-1,]\n\n## may need to remove last row, if ends on day earlier in the week\n#btc_rec_d_df &lt;- btc_rec_d_df[-nrow(btc_rec_d_df),]\n## calc % chg \nbtc_rec_d_df &lt;- btc_rec_d_df %&gt;% mutate(\n  wk_chg=BTC.CAD.Close/lag(BTC.CAD.Close)-1\n)\n## calculate some stats and make them pretty for printing\nmwkchg_calc &lt;- median(btc_rec_d_df$wk_chg, na.rm=TRUE)\nmwkchg &lt;- glue(prettyNum(mwkchg_calc*100, digits=2),\"%\")\nawkchg_calc &lt;- mean(btc_rec_d_df$wk_chg, na.rm=TRUE)\nawkchg &lt;- glue(prettyNum(awkchg_calc*100, digits=2),\"%\")\nwkchg_pctl &lt;- quantile(btc_rec_d_df$wk_chg, 0.5, na.rm=TRUE)\n## set color for mean based on above/below zero\nacolor &lt;- ifelse(awkchg_calc&gt;0,'green','red')\napos &lt;- ifelse(awkchg_calc&gt;0,0.05,-0.05)\nmpos &lt;- ifelse(mwkchg_calc&gt;0,0.05,-0.05)\n## histogram\nbtc_rec_d_df %&gt;% ggplot(aes(x=wk_chg))+geom_histogram(fill=fill_color)+\n  scale_y_continuous(expand = c(0,0))+\n  geom_vline(xintercept=mwkchg_calc, color='black', linetype='dashed', size=1)+\n  geom_vline(xintercept=awkchg_calc, color=acolor, linetype='dashed', size=1)+\n  annotate(geom='text', label=paste0(\"median: \",mwkchg), x=mwkchg_calc+mpos, y=10, color='black')+\n  annotate(geom='text', label=paste0(\"ave: \",awkchg), x=awkchg_calc+apos, y=12, color=acolor)+\n  labs(title=paste0(\"Distribution of Weekly Returns from \",day_01,\" to \",  day_02), y=\"\")\n\n\n\n\n\n\n\n\nBasically a wash:\n\nmedian of 0.27% tells us there is 50% chance of being either above or below 0.27% return on the week, with pretty even distribution on each side. Being very close to 0, doesn’t give us much hope.\nAve. return on the week (0.54%) holds some potential promise but not particularly inspiring.\n\n\n\nSide-by-Side Boxplot\n\n## check boxplot\nbtc_rec_d_df %&gt;% ggplot(aes(x=day, y=BTC.CAD.Close))+geom_boxplot(fill=fill_color)+\n  labs(\"Compare Price by Day\", x=\"\")\n\n\n\n\n\n\n\n\nSimilar conclusion to previous: looks like there might be some difference, not conclusive.\n\n\n\n\nIssues\nTwo potential (major) issues:\n\nTransaction fees, spread, etc. will likely erase this small gains - at least at the level many of us operate.\nThe gains may not be statistically significant enough to rely on.\n\nWe can assume that issue #1 is a deal-breaker, but we can dig deeper on issue #2 for fun.\n\n\nHypothesis Test\nFor this we can use paired sample t-test. This is used to compare ‘before / after’ situations within the same samples.\n\nRequirements\nFor paired t-test to be valid, we need the following:\n\nCalculate difference for each sample.\nNormally distributed data: check for normal distribution of differences.\nRun t-test with paired = TRUE.\n\n\ndifferences have already been calculated.\nnormality check: can be eye-balled in histogram above. For extra measure, check QQ plot and Shapiro-Wilk test. (Not really needed, since well over 30 samples, but what the heck)\n\n\nQQ Test\n\n## check normality\nqqnorm(btc_rec_d_df$wk_chg) ## qqplot\nqqline(btc_rec_d_df$wk_chg) ## shows line of perfect normal\n\n\n\n\n\n\n\n\nQQ plot looks solid in the mid-range, but data points drift off at the extremes (ideally dots should be right along the line).\n\n\nShapiro test\n\nshapiro.test(btc_rec_d_df$wk_chg) ## Shapiro-Wilk test for normality\n\n\n    Shapiro-Wilk normality test\n\ndata:  btc_rec_d_df$wk_chg\nW = 0.9, p-value = 0.0003\n\n\nLow p-value indicates that the data is NOT normally distributed. So that limits the reliability of a paired t-test.\nNevertheless…we’ve come this far, might as well check on statistical significance of the differences in prices between the days.\n\n\n\nPaired t-test\nPaired t.test to see if the differences in prices from one day to the other are statistically significant, in that they differ from what would be seen with a random collection of prices:\n\nt.test(data=btc_rec_d_df, BTC.CAD.Close ~ day, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  BTC.CAD.Close by day\nt = -1, df = 43, p-value = 0.2\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -870  216\nsample estimates:\nmean of the differences \n                   -327 \n\n\nLooks like we do not have statistical significance at all:\n\nhigh p-value\nwide confidence interval straddling 0`\n\nSo if this info was to be used for investment advice - which it definitely is not - the advice would be: do not try this at home. ;("
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt03-beer/index.html",
    "href": "posts/bc-liquor-market-review-2023-06-pt03-beer/index.html",
    "title": "BC Liquor Sales Analysis: Beer 2016-2023",
    "section": "",
    "text": "Code\n# function for determining how many complete yrs of data, used in \nfn_yr_qtr &lt;- function(data) {\n  # create variable to identify/remove partial yrs by counting qtrs and select yrs with 4\n  yr_qtr &lt;- data %&gt;% group_by(year, qtr) %&gt;% summarize(count=1)\n  yr_qtr &lt;- yr_qtr %&gt;% group_by(year) %&gt;% summarize(count=n()) %&gt;% filter(count==4)\n  data_cy &lt;- data %&gt;% filter(year %in% yr_qtr$year)\n  return(data_cy)\n}\nlmr_data_cy &lt;- fn_yr_qtr(lmr_data)\nThis is a continuation of a previous analysis of annual liquor sales in British Columbia and quarter-of-year patterns in BC liquor sales, based on data from British Columbia Liquor Distribution Board ‘Liquor Market Review’. The Liquor Market Review is released on a quarterly basis, covering dollar and litre sales across major alcoholic beverage categories: here we focus on BEER!\nData shown here goes back to beginning of 2016 (BC LDB fiscal yr 2016 Q4).\nAs mentioned in previous articles: my expertise is in data analysis, not the liquor industry, so the emphasis is on exploring what the data can tell us. Industry-insider context may be lacking. In the interest of promoting data analysis and learning, I am sharing most of the R code used to process the data - hence the expandable ‘Code’ options."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt03-beer/index.html#beer-sales-trends-in-bc",
    "href": "posts/bc-liquor-market-review-2023-06-pt03-beer/index.html#beer-sales-trends-in-bc",
    "title": "BC Liquor Sales Analysis: Beer 2016-2023",
    "section": "Beer Sales Trends in BC",
    "text": "Beer Sales Trends in BC\n\nOverview\nWe’ll start with an annual overview across all beer categories and then look at trends for individual categories further down below.\n\n\nCode\n## function for summarizing data on various dimensions\nfn_trend_yr_smry &lt;- function(data, grp1=year, grp2) {\n  grp1 &lt;- enquo(grp1)\n  grp2 &lt;- enquo(grp2)\n\n  # ttls for yr to use for % of ttl calcs used with various dimensions\n  trend_yr_ttl &lt;- data %&gt;% group_by(!!grp1) %&gt;% summarize(\n    ttl_netsales=sum(netsales),\n    ttl_litres=sum(litres)\n  )\n  \n  # summarize data by cal. yr & secondary dimension - if available\n  trend_yr &lt;- data %&gt;% group_by(!!grp1, !!grp2) %&gt;% summarize(\n    netsales=sum(netsales),\n    litres=sum(litres)\n  )\n  \n  # add % chg YoY, $/l\n  # determine lag based on how many items in dimension (must be consistent)\n  trend_yr &lt;- trend_yr %&gt;% ungroup()\n  if(!is.null(grp2)) {\n    grp &lt;- trend_yr %&gt;% group_by(!!grp2) %&gt;% summarize(count=n())\n    n_items &lt;- nrow(grp)\n  } else {\n    n_items &lt;- 1\n  }\n  # calc % chg YoY, by dimension if applicable\n  trend_yr &lt;- trend_yr %&gt;% mutate(\n    pc_chg_sales=netsales/lag(netsales, n=n_items)-1,\n    pc_chg_litres=litres/lag(litres, n=n_items)-1,\n    dollar_per_litre=netsales/litres,\n    pc_chg_d_per_l=dollar_per_litre/lag(dollar_per_litre)-1\n  )\n  \n  # calc cumulative chg over period - based on no. of periods and dimensions (if applciable)\n  n_yrs &lt;- max(trend_yr$year)-min(trend_yr$year)\n  trend_yr &lt;- trend_yr %&gt;% mutate(\n    pc_chg_sales_cum = netsales/lag(netsales, n=(n_yrs*n_items))-1,\n    pc_chg_lt_cum =  litres/lag(litres, n=(n_yrs*n_items))-1\n  )\n  \n  # calc % of annual total for each dimension (will be 1 for yr, if no additional dimensions)\n  trend_yr &lt;- left_join(trend_yr, trend_yr_ttl, by='year')\n  # calc %\n  trend_yr &lt;- trend_yr %&gt;% mutate(\n    litres_pc=litres/ttl_litres,\n    netsales_pc=netsales/ttl_netsales\n)\n  return(trend_yr)\n}\n\n\n\n\nCode\ntrend_yr &lt;- fn_trend_yr_smry(data=lmr_data_cy)\n\n\nBeer sales have been pretty stable over the last few years, with net $ sales peaking in 2017, recovering in 2022 in line with inflation. Overall slight downward trend, most noticeable in litre consumption.\n\n\nCode\n# sizes not ideal BUT...setting smaller (3, 5) makes it out of sync with plotly plots below\n# dual axis - not usually recommended but works ok here \n# - using ggplot because doesn't work well with ggplotly\nch_title &lt;- \"BC Beer: Net Sales $ + Litre Sales trend\"\nplot &lt;- trend_yr %&gt;% ggplot(aes(x=as.factor(year), y=netsales, group=1))+\n  geom_line(color=bar_col, size=2)+\n  geom_smooth(aes(y=netsales), method='lm', se=FALSE, color='brown', linetype='solid', size=1)+\n  geom_point(aes(y=netsales), color=bar_col, size=3)+\n  geom_line(aes(x=as.factor(year), y=litres*3), size=2, color='royalblue')+\n  geom_smooth(aes(y=litres*3), method='lm', se=FALSE, color='brown')+\n  geom_point(aes(x=as.factor(year), y=litres*3), size=3, color='royalblue')+\n  scale_y_continuous(name='net $ sales',\n                      labels=comma_format(prefix=\"$\", scale=1e-9,suffix=\"B\"), \n                     expand=expansion(mult=c(0,0.1)), \n                     limits=c(0,max(trend_yr$netsales)),\n                     sec.axis = sec_axis(trans=~./3, name='litres',\n                                         labels=comma_format(scale=1e-6, \n                                                             suffix=\"M\")))+\n  labs(title=ch_title, x=\"\")+\n  theme_classic()+\n  theme(\n    axis.title.y = element_text(color = bar_col, size=13),\n    axis.title.y.right = element_text(color = 'royalblue', size=13)\n  )\n\nplot\n\n\n\n\n\n\n\n\n\n\n\nCode\n## aggregate data by quarter\ntrend_yr_qtr &lt;- lmr_data_cy %&gt;% group_by(end_qtr_dt) %&gt;% summarize(\n  netsales=sum(netsales),\n  litres=sum(litres)\n) \n\n\nBreaking out litre sales by quarter confirms the steady downward trend within seasonal cycles.\n\n\nCode\n# plotly charts don't respond to in-line size settings\nch_title &lt;- \"BC Beer Litre Sales by Qtr, with Trend\"\nplot &lt;- trend_yr_qtr %&gt;% ggplot(aes(x=end_qtr_dt, y=litres, group=1))+\n  geom_line(color=bar_col, size=1.4)+\n  #geom_point(aes(y=litres), color=bar_col, size=3)+\n  geom_smooth(method='lm', se=FALSE)+\n  scale_y_continuous(labels=comma_format(scale=1e-6,suffix=\"M\"), \n                     expand=expansion(mult=c(0,0.1)), \n                     limits=c(0,max(trend_yr_qtr$litres)))+\n  labs(title=ch_title, x=\"\")\nggplotly(plot)\n\n\n\n\n\n\nCode\n# tried to use this code suggested by chatgpt for more appropriate sizing\n# size worked but left gap below; layout settings didn't do anything\n#ggplotly(plot, width=440, height=260) %&gt;% \n#  layout(autosize = TRUE, margin = list(l = 0, r = 0, b = 0, t = 0, pad = 0))\n\n\nThe flat trend/recent increase in net $ sales vs downward trend in litre sales shows up in gradual uptrend in net $ per litre sales. Brewers are selling fewer litres but getting more money for each of them (on average).\n\n\nCode\nch_title &lt;- \"BC Beer: Net $/Litre trend\"\nplot &lt;- trend_yr %&gt;% ggplot(aes(x=as.factor(year), y=dollar_per_litre, group=1))+\n  geom_line(color=bar_col, size=2)+\n  geom_smooth(aes(y=dollar_per_litre), method='lm', se=FALSE, color='brown', linetype='solid', size=1)+\n  geom_point(aes(y=dollar_per_litre), color=bar_col, size=3)+\n  scale_y_continuous(name='Net $/litre',\n                      labels=comma_format(prefix=\"$\"), \n                     expand=expansion(mult=c(0,0.1)), \n                     limits=c(0,max(trend_yr$dollar_per_litre)))+\n  labs(title=ch_title, x=\"\")+\n  theme_classic()+\n  theme(\n    axis.title.y = element_text(color = bar_col, size=13)\n  )\n\nggplotly(plot)\n\n\n\n\n\n\n\n\nSales by Category\nBC LDB puts beer into 3 major categories for purposes of the Liquor Market Report:\n\nDomestic - BC Beer: beer produced in BC.\nDomestic - Other Province Beer: beer produced in Canada outside BC.\nImport Beer: beer imported from outside Canada.\n\n\n\nCode\n# summarize data by year and major category - using function\ntrend_yr_cat &lt;- fn_trend_yr_smry(data=lmr_data_cy, grp2=category)\n\n## simplify category names\n#unique(trend_yr_cat$category)\ntrend_yr_cat &lt;- trend_yr_cat %&gt;% mutate(\n  category=case_when(\n    category=='Domestic - BC Beer' ~ 'BC',\n    category=='Domestic - Other Province Beer' ~ 'Other Prov.',\n    category=='Import Beer' ~ 'Import'\n  )\n)\n\n\n\nLitre sales\nLooking at litre consumption for these 3 main categories:\n\nvolume drops for BC and Import beer, with other province beer actually increasing.\nin % terms, litre market share for BC producers remained steady just under 80%, while other provinces took their market share out of imports.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear-over-Year % Changes\nThe year-over-year dynamics in litre consumption stand out even clearer when looking at % change:\n\nmostly stable for BC producers.\nBIG gains by other Canadian provinces - as high as almost 40% growth in 2021. Pandemic effect?\nImports as the big losers, with over 30% loss in 2021. Possibly related to pandemic and related supply chain issues?\n\n\n\nCode\n# YoY % chg chart\nch_title &lt;- \"Year-over-Year % change in Litre Sales\"\nplot &lt;- trend_yr_cat %&gt;% \n  ggplot(aes(x=year, y=pc_chg_litres, text=paste0(year,\": \",\n                                                round(pc_chg_litres*100),\"%\")))+\n  geom_col(aes(fill=pc_chg_litres&gt;0), show.legend=FALSE) +\n  scale_fill_manual(values=c(\"TRUE\"=posneg_col[2], \"FALSE\"=posneg_col[1])) +\n  facet_grid(.~category)+\n  scale_y_continuous(labels=percent_format())+\n  geom_hline(yintercept=0)+\n  labs(title=ch_title, x=\"\", y=\"Litre sales - % chg\")+\n  theme_bw()+\n  theme(panel.grid = element_blank(),\n        strip.background = element_rect(fill=\"lightblue\"),\n        legend.position = \"none\")\nggplotly(plot, tooltip=\"text\")\n\n\n\n\n\n\n\n\nCumulative % Change\nAll these year-over-year changes add up - especially for Other Province beer and Imports:\n\nOther Province beer almost doubled over the period, with 90% growth!\nImports dropped in half\nBC-produced beer saw modest decline.\n\n\n\nCode\nch_title &lt;- paste0(\"Total % chg: \", min(trend_yr_cat$year), \" - \", max(trend_yr_cat$year))\n\nplot &lt;- trend_yr_cat %&gt;% ggplot(aes(x=fct_rev(category), y=pc_chg_lt_cum)) +\n  geom_col(aes(fill=pc_chg_lt_cum &gt; 0), show.legend = FALSE) +\n  scale_fill_manual(values = c(\"TRUE\" = posneg_col[2], \"FALSE\" = posneg_col[1])) +\n   geom_text(\n      aes(label=percent(pc_chg_lt_cum, accuracy=1),\n      hjust=0.5,\n      vjust=ifelse(pc_chg_lt_cum&gt;0, -0.5, 1.2)),\n      size=3)+\n  scale_y_continuous(limits=c(-1,1), expand=expansion(add=c(0.2,0.2)),\n                     labels=percent_format(), breaks=c(-1,0,1)) +\n  geom_hline(yintercept=0, linetype='solid') +\n  labs(title=ch_title, x=\"\", y=\"\")+\n  theme(axis.ticks.x = element_blank(),\n        axis.line = element_blank(),\n        panel.border = element_rect(fill=NA, size=0.3))\nplot\n\n\n\n\n\n\n\n\n\nClearly, breweries in other provinces are winning that battle over imported beer for the roughly 20% of volume not accounted for by BC-produced beer.\n\n\nWhere do the dollars go?\nNot surprisingly, the shift in volume from Imports to Other Provinces is reflected in dollar sales as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBC breweries have a similarly stable share of dollar sales, as with litre sales, although at a slightly lower % share: ~75%. Other provinces are progressively winning the battle for the remaining ~25%, more than doubling their share of $ during the period.\n\n\n\nRegional View\nBC LDB provides data for importers by country, allowing us to take a closer look at dynamics within this segment, relative to Other Provinces. (BC producers removed to zoom in on external producers)\n\n\nCode\n# need a new field for region - need to go back to lmr_data\nlmr_data_reg &lt;- lmr_data_cy %&gt;% mutate(\n  region=ifelse(str_detect(subcategory, 'Domestic - BC'),'Canada - BC',\n                ifelse(str_detect(subcategory, 'Domestic - Other'), 'Canada - Other', subcategory))\n)\n# remove BC to focus on regions outside BC\nlmr_data_reg_xbc &lt;- lmr_data_reg %&gt;% filter(\n  region!='Canada - BC'\n)\n# summarize data by year and region\ntrend_yr_reg &lt;- fn_trend_yr_smry(data=lmr_data_reg_xbc, grp2=region)\n  \n## remove 'beer' from region names for brevity\ntrend_yr_reg &lt;- trend_yr_reg %&gt;% mutate(\n  region=str_replace_all(region, ' Beer','')\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome interesting dynamics show above:\n\noverall downward trend since 2017.\nCanadian provinces other than BC have been the gainers while other regions, most notable Europe, Mexico, US have seen declines.\n\n\n\nRegional breakout by litres, % change, $/litre\nWe can do a deeper dive on regional differences by comparing not only litre sales but also % change in litre sales over time, and $/litre (proxy for price):\nBC producers excluded to zoom in on external producer dynamics.\n‘Other Country’ excluded for simplicity, due to tiny amount of sales.\n\n\nCode\nexcl &lt;- \"Other Country\" # excluding 'other country' as minimal\nch_title &lt;- \"% Change in Annual Litre Sales over Period vs Latest Yr Litres\"\nplot &lt;- trend_yr_reg %&gt;% filter(year==max(trend_yr_reg$year) & region!=excl) %&gt;% \n  ggplot(aes(x=litres, y=pc_chg_lt_cum, color=region, size=dollar_per_litre))+\n           geom_point()+\n  scale_x_continuous(labels=label_comma(), expand=c(0.02,0.05))+\n  scale_y_continuous(labels=percent_format())+\n  scale_size(range = c(1,6)) + \n  geom_hline(yintercept = 0, linetype='solid')+\n  theme(axis.line.x = element_blank())+\n  labs(title= ch_title,  \n       y=\"% chg in annual litre sales over period\", \n       x=\"litres sold in most recent year\")\nggplotly(plot)\n\n\n\n\n\n\n\nWith BC producers removed, Canada - Other is by far the most litres sold, by far the cheapest (smallest dot), and hugely positive growth in litre sales over the period.\nAll other producers have seen negative growth in litres sold, with US and Mexico particularly hard hit.\nUS decline is particularly noteworthy, dropping over 90%! Possibly due to more US brands being produced within Canada, such as Budweiser being brewed in Canada by Labatt (AB InBev) and Coors being brewed by Molson (now Molson Coors).\n\nFor a year-by-year view…\n\n\nCode\n# faceted view by year since animation doesn't work\nch_title &lt;- \"Year by Year Changes in Litres Sold\"\nplot_yr &lt;- trend_yr_reg %&gt;% filter(region!=excl) %&gt;%\n   ggplot(aes(x=litres, y=pc_chg_litres, color=region, size=dollar_per_litre))+\n            geom_point()+\n   facet_wrap(~year)+\n   scale_x_continuous(labels=label_comma()) +\n   scale_y_continuous(labels=percent_format()) +\n   scale_size(range = c(1,8)) +\n   geom_hline(yintercept = 0, linetype='solid') +\n   labs(title = ch_title, y=\"% chg over period\", \n        x=paste0(\"annual litres sold: {frame_time}\"))\nplot_yr\n\n\n\n\n\n\n\n\n\n\n\nBeer: Wrap-up and Next Up\nSome interesting dynamics happening with beer sales in BC, with a couple of the main themes being:\n\ngradual downward trend in litre sales\nstable market share for BC producers, around 80%.\nstrong growth (90%) in litre sales from producers in other Canadian provinces almost entirely at the expense of beer imported from outside Canada.\n\n\nNext Up\nNext article will drill into details of BC producer categories: Commercial beer, Regional beer, Microbrew beer.\n\n\n\nFootnotes\nNotes on ‘net $ sales’:\n\nthe report says “Net dollar value is based on the price paid by the customer and excludes any applicable taxes.”\ncalculating average net dollar value per litre for beverage categories gives unrealistically low numbers compared to retail prices in BC liquor stores. (Beer at average $4/litre? Not even the cheapest beer on the BC Liquor Stores website.)\nthere is likely additional factors related to BC LDB pricing structure, wholesaling, etc.\nbest to consider average net dollar value per litre referred to below as relative indicator."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-12-pt04-beer-category/index.html",
    "href": "posts/bc-liquor-market-review-2023-12-pt04-beer-category/index.html",
    "title": "BC Beer Sales Analysis: Commercial, Regional, Micro Brew 2016-2023",
    "section": "",
    "text": "Code\n# refine data for current purposes\n# save orig lmr_data\nlmr_data_orig &lt;- lmr_data\n# add filter for Canadian categorized beer only\nlmr_data &lt;- lmr_data %&gt;% filter(str_starts(category, 'Domestic'))\n# clean-up, simplify cat/subcat names\nlmr_data &lt;- lmr_data %&gt;% mutate(\n  category = str_remove_all(category, \"Domestic - \"),\n  category = str_remove(category, \" Beer\"),\n  subcategory = str_remove_all(subcategory, \"Domestic - \"),\n  subcategory = str_remove_all(subcategory, \"BC \"),\n  subcategory = str_remove(subcategory, \"Other Province \"),\n  subcategory = str_remove_all(subcategory, \" Beer\")\n) \n# further renaming for simplification - easier to work with category code carried \n# over from previous (and arguably 'source' and 'category' more applicable)\nlmr_data &lt;- lmr_data %&gt;% rename(\n  source = category,\n  category = subcategory\n)"
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-12-pt04-beer-category/index.html#intro",
    "href": "posts/bc-liquor-market-review-2023-12-pt04-beer-category/index.html#intro",
    "title": "BC Beer Sales Analysis: Commercial, Regional, Micro Brew 2016-2023",
    "section": "Intro",
    "text": "Intro\nA look at data on BC beer sales by brewery type for Canadian breweries, taken from the quarterly British Columbia Liquor Distribution Board ‘Liquor Market Review’. This is a continuation of a previous analysis of annual liquor sales in British Columbia and quarter-of-year patterns in BC liquor sales, and BC Liquor Sales Analysis: Beer. Focus for this analysis is specifically on the brewery categories broken out in the report:\n\n‘commercial’\n‘regional’\n‘micro brew’\n\nData shown here goes back to beginning of 2016 (BC LDB fiscal yr 2016 Q4).\n\n\n\n\n\n\nNote\n\n\n\nAs mentioned in previous articles: my expertise is in data analysis, not the liquor industry, so the emphasis is on exploring what the data can tell us. Industry-insider context may be lacking. In the interest of promoting data analysis and learning, I am sharing most of the R code used to process the data - hence the expandable ‘Code’ options."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-12-pt04-beer-category/index.html#beer-sales-trends-canadian-beer-by-category",
    "href": "posts/bc-liquor-market-review-2023-12-pt04-beer-category/index.html#beer-sales-trends-canadian-beer-by-category",
    "title": "BC Beer Sales Analysis: Commercial, Regional, Micro Brew 2016-2023",
    "section": "Beer Sales Trends: Canadian Beer by Category",
    "text": "Beer Sales Trends: Canadian Beer by Category\nBC LDB LMR breaks out Canadian beer based on location of production: BC-produced beer and ‘Other province’-produced beer. For each of these, it provides further breakdown by brewery size. Here are the definitions, taken from the ‘Glossary’ section of the report:\n\nCommercial Beer: Breweries with Annual Production over 350,000HL\nRegional Beer: Breweries with Annual Production over 15,000HL and up to 350,000HL\nMicro Brew Beer: Breweries with Annual Production up to 15,000HL (aka ‘craft’ beer)\n\nAs you can see, these categories are based strictly on production volume, which seems a bit of a crude measurement. I believe this is for determining tax rates. In any case, we work with what we have. ;)\n\nSales by Category\n\n\nCode\n# remove partial yrs by counting qtrs and identifying those with 4\nyr_qtr &lt;- lmr_data %&gt;% group_by(year, qtr) %&gt;% summarize(count=1)\nyr_qtr &lt;- yr_qtr %&gt;% group_by(year) %&gt;% summarize(count=n()) %&gt;% filter(count==4)\n# remove partials yrs\nlmr_data &lt;- lmr_data %&gt;% filter(year %in% yr_qtr$year)\n\n# calculate annual ttls for % of ttl calculations later\ntrend_yr_ttl &lt;- lmr_data %&gt;% group_by(year) %&gt;% summarize(\n  ttl_netsales=sum(netsales),\n  ttl_litres=sum(litres)\n)\n\n\n\n\nCode\n# summarize data by year and category\ntrend_yr_cat &lt;- lmr_data %&gt;% group_by(year, category) %&gt;% summarize(\n  netsales=sum(netsales),\n  litres=sum(litres)\n) %&gt;% filter(year %in% yr_qtr$year) \n\n# join with trend_yr_cat to get yr total cols\ntrend_yr_cat &lt;- left_join(trend_yr_cat, trend_yr_ttl, by='year')\n# calc %\ntrend_yr_cat &lt;- trend_yr_cat %&gt;% mutate(\n  litres_pc=litres/ttl_litres,\n  netsales_pc=netsales/ttl_netsales\n)\n\n# add % chg YoY, $/l\n# ungroup first; use n=3 to lag 3 rows to match categories\ntrend_yr_cat &lt;- trend_yr_cat %&gt;% ungroup() %&gt;% mutate(\n  pc_chg_sales=netsales/lag(netsales, n=3)-1,\n  pc_chg_litres=litres/lag(litres, n=3)-1,\n  dollar_per_litre=netsales/litres,\n  pc_chg_d_per_l=dollar_per_litre/lag(dollar_per_litre, n=3)-1\n)\n\n\n\nLitre sales\nLooking at litre consumption for these 3 main categories:\n\nas noted previously in BC Liquor Sales Analysis: Beer, overall beer sales are fairly stable (including imported beer not shown here), with slow downward trend.\nmost of the downward trend in volume sales is accounted for by lower commercial (big brewery) beer sales.\nin % terms, litre market share has picked up for regional breweries (16% to 20%) at the expense of commercial beer, while the smaller micro brew category has stabilized in recent yrs at ~12%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNet $ Sales\nNet $ sales tells a pretty similar story, in terms of breakdown and trends, especially when considering:\n\ninflation is bound to account for upward drift in overall $ sales (all things equal).\nslightly higher share of $ sales for regional and micro brews, since these beers are more expensive than commercial brewery beer.\n\nIt is interesting to note that commercial breweries have stayed level, in $ terms, despite litre volume decreases. Price increases have almost exactly offset the volume declines."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-12-pt04-beer-category/index.html#sales-by-category-by-source",
    "href": "posts/bc-liquor-market-review-2023-12-pt04-beer-category/index.html#sales-by-category-by-source",
    "title": "BC Beer Sales Analysis: Commercial, Regional, Micro Brew 2016-2023",
    "section": "Sales by Category by Source",
    "text": "Sales by Category by Source\nLet’s see how things break-out by source (BC or other province) for these categories.\n\n\nCode\n## % share of categories by src -&gt; see what composition is by source\n## get annual ttls by src (all categories)\ntrend_yr_src_ttl &lt;- lmr_data %&gt;% group_by(year, source) %&gt;% summarize(\n    ttl_src_netsales=sum(netsales),\n    ttl_src_litres=sum(litres)\n    )\n\n## get data for category by source\ntrend_yr_src_cat &lt;- lmr_data %&gt;% \n  group_by(year, source, category) %&gt;% summarize(\n    netsales=sum(netsales),\n    litres=sum(litres))\n\n# calc % of yr sales by category\n# join with trend_yr_cat to get yr total cols\ntrend_yr_src_cat &lt;- left_join(trend_yr_src_cat, trend_yr_src_ttl, \n                              by=c('year','source'))\n# calc %\ntrend_yr_src_cat &lt;- trend_yr_src_cat %&gt;% mutate(\n  litres_pc=litres/ttl_src_litres,\n  netsales_pc=netsales/ttl_src_netsales\n)\n\n# ungroup for % chg calcs over lag\n# set lag based on how far to look back for comp\nlag &lt;- 6\ntrend_yr_src_cat &lt;- trend_yr_src_cat %&gt;% ungroup %&gt;% mutate(\n  pc_chg_sales=netsales/lag(netsales, n=lag)-1,\n  pc_chg_litres=litres/lag(litres, n=lag)-1,\n  dollar_per_litre=netsales/litres,\n  pc_chg_d_per_l=dollar_per_litre/lag(dollar_per_litre, n=lag)-1\n)\n\n\n\nCategory Breakdown by Source\nSales in all the categories are driven by BC producers, with out-of-province producers only having an noticeable impact on commercial brewery sales.\n\nNotably, beer from other province commercial breweries appears to have taken share from BC commercial breweries.\nPresumably due to geographical reallocation of production by large multi-national breweries?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear-over-Year % Changes by Category, Source\nBreaking out the year-over-year % changes, we see that:\n\nBC commercial beer has small but steady declines.\nOther province commercial beer has had some surges in recent yrs, although stabilized in the last couple of years.\nBC regional breweries have had some good growth years without much negative growth.\nBC micro brews had some strong years in 2017-18 and 2021 (after 2020 pandemic) and growth has slowed/ended.\nOther province regional and micro brews have had some large swings (mostly declines), but these are relative to a base too small to matter.\n\n\n\nCode\ntrend_yr_src_cat %&gt;% ggplot(aes(x=year, y=pc_chg_litres, fill=category))+\n  geom_col(show.legend = FALSE)+\n  facet_grid(source~.~category)+\n  scale_y_continuous(labels=percent_format())+\n  geom_hline(yintercept=0)+\n  labs(title='Year-over-Year % change in Litre Sales', x=\"\", y=\"Litre sales - % chg\")+\n  theme_bw()+\n  theme(panel.grid = element_blank(),\n        strip.background = element_rect(fill=\"lightblue\"))\n\n\n\n\n\n\n\n\n\n\n\nCumulative % Change over the Years\n\n\nCode\n# calculate cumulative change across available yrs\n# need to lag by n yrs in dataset x n categories (or items to be more general)\nn_yrs &lt;- max(trend_yr_src_cat$year)-min(trend_yr_src_cat$year)\nn_items &lt;- length(unique(trend_yr_src_cat$category)) # no. of categories\nn_items &lt;- n_items*length(unique(trend_yr_src_cat$source)) # x no. of sources\n# lag calculation based on n_yrs x n_items\ntrend_yr_src_cat &lt;- trend_yr_src_cat %&gt;% mutate(\n  pc_chg_lt_cum =  litres/lag(litres, n=(n_yrs*n_items))-1\n)\n\n\nHere we see the biggest cumulative change in other province commercial beer, more than doubling in litre sales during the period, accompanied by drop-off in BC commercial beer volume.\n\nmeanwhile, strong growth overall for BC regional and micro brew beers.\n\n\n\nCode\nch_title &lt;- paste0(\"Total % chg: \", min(trend_yr_src_cat$year), \" - \", \n                   max(trend_yr_src_cat$year))\n# bar chart\ntrend_yr_src_cat %&gt;% ggplot(aes(x=category, y=pc_chg_lt_cum, fill=category))+geom_col()+\n  scale_y_continuous(labels=percent_format())+\n  geom_hline(yintercept=0, linetype='solid')+\n  facet_grid(source~.)+\n  #coord_flip()+\n  labs(title=ch_title, x=\"\", y=\"\")+\n  theme(axis.ticks.x = element_blank(),\n        legend.position = \"none\")"
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-12-pt04-beer-category/index.html#conclusion",
    "href": "posts/bc-liquor-market-review-2023-12-pt04-beer-category/index.html#conclusion",
    "title": "BC Beer Sales Analysis: Commercial, Regional, Micro Brew 2016-2023",
    "section": "Conclusion",
    "text": "Conclusion\nNothing too surprising or dramatic here, but a good overview of the dynamics of the BC beer market across 3 categories identified by BC LDB for Canadian producers.\n\nNext\nNo further plans to explore the BC LDB data in this format, at least not for a while. Planning to turn my attention to a more interactive, sustainable dashboard that folks can use to explore the LDB Quarterly Market Review as they see fit. Stay tuned!\n\n\nFootnotes\nNotes on ‘net $ sales’:\n\nthe report says “Net dollar value is based on the price paid by the customer and excludes any applicable taxes.”\ncalculating average net dollar value per litre for beverage categories gives unrealistically low numbers compared to retail prices in BC liquor stores. (Beer at average $4/litre? Not even the cheapest beer on the BC Liquor Stores website.)\nthere is likely additional factors related to BC LDB pricing structure, wholesaling, etc.\nbest to consider average net dollar value per litre referred to below as relative indicator."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html",
    "href": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html",
    "title": "BC Liquor Sales Analysis: Annual Trends 2016-2022",
    "section": "",
    "text": "British Columbia Liquor Distribution Board releases its ‘Liquor Market Review’ on a quarterly basis, covering dollar and litre sales across major alcoholic beverage types of beer, wine, spirits, and ‘refreshment beverages’ (ciders, coolers).\nThis is a combined look using reports going back to 2015, when the current format was started.\nNote: my expertise is in data analysis, not the liquor industry, so the emphasis is on exploring what the data can tell us. Industry-insider context may be lacking. In the interest of promoting data analysis, I am sharing most of the R code used to process the data - hence the expandable ‘Code’ options."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html#annual-trends-all-beverage-types",
    "href": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html#annual-trends-all-beverage-types",
    "title": "BC Liquor Sales Analysis: Annual Trends 2016-2022",
    "section": "Annual Trends: All Beverage Types",
    "text": "Annual Trends: All Beverage Types\n(Based on calendar year, using earliest and most recent full year avail.)\n\n$ Sales\n\n\nCode\n# get full years for comparison\ndrop_yr &lt;- c(2015,2023) ## drop partial yrs for simplicity\ntrend_yr &lt;- lmr_data %&gt;% filter(year&gt;drop_yr[1] & year&lt;drop_yr[2]) %&gt;% \n  group_by(year) %&gt;% summarize(netsales=sum(netsales),\n                              litres=sum(litres))\n\n# add % chg YoY, $/l\ntrend_yr &lt;- trend_yr %&gt;% mutate(\n  pc_chg_sales=netsales/lag(netsales)-1,\n  pc_chg_litres=litres/lag(litres)-1,\n  dollar_per_litre=netsales/litres,\n  pc_chg_d_per_l=dollar_per_litre/lag(dollar_per_litre)-1\n)\n\n\nThere has been general upward drift in net sales dollars in recent years, rising to $3.7 billion in 2022:\n\n\nCode\nch_title &lt;- \"Net Sales $ Trends - All Types\"\n## assign plot definition to variable for use with plotly\nplot &lt;- trend_yr %&gt;% ggplot(aes(x=year, y=netsales, group=1))+\n  geom_line(color=bar_col, linewidth=2)+\n  geom_point(aes(y=netsales), color=bar_col, size=3)+\n  scale_x_continuous(breaks=trend_yr$year)+ ## ensure each value shown in x-axis\n  scale_y_continuous(labels=comma_format(prefix=\"$\", scale=1e-9,suffix=\"B\"), \n                     expand=expansion(mult=c(0,0.1)), limits=c(0,max(trend_yr$netsales)))+\n  labs(title=ch_title,y=\"\",x=\"\")+\n  theme_classic()\nggplotly(plot) ## convert to plotly for interactivity\n\n\n\n\n\n\n\nYear-over-Year % Change: ($ Sales)\nThere is an out-sized year-over-year % increase in 2020, first year of the covid-19 pandemic:, with growth nearing 6%:\n\n\nCode\n# chart % chg sales YoY\nch_title &lt;- \"Net Sales: Year-over-Year % change\"\n# convert year to integer to identify first yr for filtering, since no number for % chg\nmin_yr &lt;- min(as.integer(as.character(trend_yr$year)))\n## plot - filtered to exclude first yr\nplot &lt;- trend_yr %&gt;% filter(year!=min_yr) %&gt;% \n  ggplot(aes(x=year, y=pc_chg_sales))+\n  geom_col(fill=bar_col)+\n  geom_hline(yintercept=mean(trend_yr$pc_chg_sales, na.rm=TRUE), linetype='dotted')+\n  geom_hline(yintercept=median(trend_yr$pc_chg_sales, na.rm=TRUE), linetype='dotted', color='lightblue')+\n  scale_x_continuous(breaks=trend_yr$year)+\n  scale_y_continuous(labels=percent_format(), expand=expansion(mult=c(0,0.1)))+\n  labs(title=ch_title,y=\"\",x=\"\")+\n   theme(axis.ticks.x = element_blank()) \nggplotly(plot)\n\n\n\n\n(black line = average, blue line = median)\n\n\nBy far the biggest YoY change was … pandemic time! Prior to 2020, annual increases were slowing down, while total still increasing. In 2022, YoY growth has returned to pre-2020 levels, close to the median (light blue line).\n\n\n\nVolume (Litres)\nSales in litres have been essentially flat in recent years, averaging 457 million litres per year:\n\n\nCode\nch_title &lt;- \"Litre Volume Trends - All Types\"\nplot &lt;- trend_yr %&gt;% \n  ggplot(aes(x=as.factor(year), y=litres, group=1))+\n  #geom_col(fill=bar_col)+\n  geom_line(color=bar_col, size=2)+\n  geom_point(aes(y=litres), color=bar_col, size=3)+\n  geom_hline(yintercept=mean(trend_yr$litres), linetype='dotted')+\n  scale_y_continuous(labels=comma_format(scale=1e-6,suffix=\"M\"), expand=expansion(mult=c(0,0.1)), limits=c(0,max(trend_yr$litres)))+\n  labs(title=ch_title,y=\"\",x=\"\")+\n  theme_classic() \nggplotly(plot)\n\n\n\n\n\n\nLitre volume has remained relatively steady, with a peak during the 2020 pandemic at 470 million litres. Volumes are slowing down, although 2022 still above 2019.\n\nYear-over-Year % Change: Volume (Litres)\nBiggest year-over-year jump of ~4% in 2020, with declines accelerating from 2021 to 2022.\n\n\nCode\n# chart % chg liters YoY\nch_title &lt;- \"Litre Vol.: Year-over-Year % change\"\nplot &lt;- trend_yr %&gt;% filter(year!=min_yr) %&gt;% ## filter first yr (calc above) since no value\n  ggplot(aes(x=as.factor(year), y=pc_chg_litres))+\n  geom_col(fill=bar_col)+\n  geom_hline(yintercept=mean(trend_yr$pc_chg_litres, na.rm=TRUE), linetype='dotted')+\n  scale_y_continuous(labels=percent_format(), expand=expansion(mult=c(0.1,0.1)))+\n  geom_hline(yintercept=0)+\n  labs(title=ch_title,y=\"\",x=\"\")+\n   theme(axis.ticks.x = element_blank()) \nggplotly(plot)\n\n\n\n\n\n\nAfter strong growth of 4% in 2020, 2022 saw by far the largest decrease in volume consumption in 6 years, down 2%.\n\n\n\n$/Litre\nLooking at trends in $/litre provides a proxy for ‘price’ and gives a sense of how overall prices have changed over time. (see footnote for caveats.) No surprise: steadily upward, increasing with inflation:\n\n\nCode\n# chart $/ltr\nch_title &lt;- \"$/Litre Trends - All Types\"\nplot &lt;- trend_yr %&gt;% \n  ggplot(aes(x=as.factor(year), y=dollar_per_litre, group=1))+\n  #geom_col(fill=bar_col)+\n  geom_line(color=bar_col, size=2)+\n  geom_point(aes(y=dollar_per_litre), color=bar_col, size=3)+\n  scale_y_continuous(labels=label_comma(prefix=\"$\", accuracy=0.01), expand=expansion(mult=c(0,0.1)), limits=c(0, max(trend_yr$dollar_per_litre)))+\n  labs(title=ch_title,y=\"\",x=\"\")+\n  theme_classic()\nggplotly(plot)\n\n\n\n\n\n\n$/litre has trended up to $8 by 2022. (this may seem unrealistically low - see footnote for caveats and use as directional information.)\n\nYear-over-Year % Change: $/Litre\nWith relatively flat litre volume sales, the increase in dollar volume is driven by increase in sales $ per litre. Dollar per litre increases have been fairly steady, around 2%, until 2021. This corresponds with overall increase in inflation, accelerating through 2021 and 2022:\n\n\nCode\n# chart % chg $/liters YoY\nch_title &lt;- \"$/Litre: Year-over-Year % change\"\nplot &lt;- trend_yr %&gt;% filter(year!=min_yr) %&gt;% \n  ggplot(aes(x=as.factor(year), y=pc_chg_d_per_l))+\n  geom_col(fill=bar_col)+\n  geom_hline(yintercept = mean(trend_yr$pc_chg_d_per_l, na.rm=TRUE), linetype='dotted')+\n  scale_y_continuous(labels=percent_format(), expand=expansion(mult=c(0,0.1)))+\n  labs(title=ch_title,y=\"\",x=\"\")+\n   theme(axis.ticks.x = element_blank()) \nggplotly(plot)\n\n\n\n\n\n\n$/litre increases look exponential from here, but would be expected to level off as inflation slows down."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html#annual-trends-by-beverage-type",
    "href": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html#annual-trends-by-beverage-type",
    "title": "BC Liquor Sales Analysis: Annual Trends 2016-2022",
    "section": "Annual Trends: By Beverage Type",
    "text": "Annual Trends: By Beverage Type\nAs noted, the BC LMR provides a breakdown of sales by four major beverage types: beer, refreshment beverages, spirits, wine.\n\nBy Type: $ Sales\n\n\nCode\n# annual data by type\ndrop_yr &lt;- c(2015,2023) ## drop partial yrs for simplicity\ntrend_yr_cat &lt;- lmr_data %&gt;% filter(year&gt;drop_yr[1] & year&lt;drop_yr[2]) %&gt;% group_by(year, type) %&gt;% summarize(\n  netsales=sum(as.numeric(netsales)),\n  litres=sum(as.numeric(litres))\n) \ntrend_yr_cat &lt;- trend_yr_cat %&gt;% ungroup() %&gt;% mutate(\n  dollar_per_litre=netsales/litres,\n  pc_chg_sales=netsales/lag(netsales, n=4)-1, # yoy change by beverage type\n  pc_chg_litre=litres/lag(litres, n=4)-1,\n  pc_chg_dollar_per_l=dollar_per_litre/lag(dollar_per_litre, n=4)-1\n)\n## get subset of trend_yr data to add annual totals to trend_yr_cat for % of ttl calculations\ntrend_yr_yr &lt;- trend_yr %&gt;% select(year, netsales, litres) %&gt;% rename(\n  netsales_ttl=netsales,\n  litres_ttl=litres\n)\ntrend_yr_cat &lt;- left_join(trend_yr_cat, trend_yr_yr, by='year') %&gt;% mutate(\n  pc_ttl_sales=netsales/netsales_ttl,\n  pc_ttl_litres=litres/litres_ttl\n)\n\n\nFor $ sales, wine and beer are the largest types (well over 50%), followed by spirits, with refreshment beverages as distant fourth:\n\n\nCode\n# annual sales by type (stack)\nch_title &lt;- \"Net $ Sales by Type\"\n# order type by netsales for chart\ntrend_yr_cat$type &lt;- fct_reorder(trend_yr_cat$type, trend_yr_cat$netsales)\ntrend_yr_cat$year &lt;- as.factor(trend_yr_cat$year) ## foctar works better for bar charts\n\nplot &lt;- trend_yr_cat %&gt;% ggplot(aes(x=year, y=netsales, fill=type))+\n  geom_col()+\n  scale_y_continuous(labels=label_comma(prefix=\"$\", scale=1e-9,suffix=\"B\"), expand=expansion(mult = c(0,0.1)))+\n  scale_fill_manual(values=type_color)+\n  labs(title=ch_title, x=\"\",y=\"\", fill='')+ ## setting fill='' removes title from legend\n  theme(axis.ticks.x = element_blank())\nggplotly(plot)\n\n\n\n\n\n\n\nBy Type: $ Sales % Breakdown\nLooking at the overall % breakdown, we can more easily spot changes in composition over time:\n\n\nCode\nch_title &lt;- \"Net $ Sales by Type, % of Total\"\nplot &lt;- trend_yr_cat %&gt;% ggplot(aes(x=year, y=pc_ttl_sales, fill=type))+\n  geom_col()+ ## could use same as prev chart with position='fill' but wanted numbers for chart\n  scale_y_continuous(labels=percent_format(), expand=expansion(mult = c(0,0.1)))+\n  scale_fill_manual(values=type_color)+\n  labs(title=ch_title, x=\"\",y=\"\", fill='')+\n  theme(axis.ticks.x = element_blank(),\n        legend.title = element_blank())\nggplotly(plot)\n\n\n\n\n\n\nPercentage breakdown shows wine and beer with bulk of market share, BUT…beer particularly fading in favour of refreshment beverages and spirits, with the former showing strongest relative growth. Beer has gone from 36% of total $ sales in 2016 to under 30% in 2022, while refreshment beverages have gone from under 6% to close to 12%.\n\n\nBy Type: $ Sales Year-over-Year % Change\nRefreshment Beverages had dolllar sales growth of over 20% for 3 years in a row:\n\n\nCode\nch_title &lt;- \"% Chg in Net $ Sales, Year-over-Year by Type\"\n## plot - filtered to exclude lowest yr, calculated above\nplot &lt;- trend_yr_cat %&gt;% filter(year!=min_yr) %&gt;%\n  ggplot(aes(x=year, y=pc_chg_sales))+\n  geom_col(fill=bar_col)+\n  geom_hline(yintercept = 0)+\n  facet_grid(.~type)+\n  scale_y_continuous(labels=percent_format())+\n  theme(strip.background = element_rect(fill = bar_col)) +\n  theme(strip.text=element_text(color='white'))+\n  labs(title=ch_title, x=\"\",y=\"\")+\n  theme(axis.text.x=element_text(hjust=0, vjust=0.5, angle=90),\n        axis.ticks.x = element_blank()\n        ,panel.border = element_rect(fill=NA)\n        ) \nggplotly(plot)\n\n\n\n\n\n\nDuring the pandemic in 2020, Refreshment Beverages had a surge in growth, on top of an already strong trend from previous years. Spirits also had a significant increase - around 10% year-over-year growth.\nBeer is the only beverage type with negative growth during the period, through 2019-2021, although recovered in 2022.\n\n\n\nBy Type: Volume (Litres)\nLooking at litre sales confirms growth of Refreshment Beverages:\n\n\nCode\n# annual litres by category (stack)\nch_title &lt;- \"Volume (Litres) by Type\"\n# order type by netsales for chart\ntrend_yr_cat$type &lt;- fct_reorder(trend_yr_cat$type, trend_yr_cat$litres)\n\nplot &lt;- trend_yr_cat %&gt;% ggplot(aes(x=year, y=litres, fill=type))+\n  geom_col()+\n  scale_y_continuous(labels=label_comma(scale=1e-6,suffix=\"M\"), expand=expansion(mult = c(0,0.1)))+\n  scale_fill_manual(values=type_color)+\n  labs(title=ch_title, x=\"\",y=\"\", fill='')+\n  theme(axis.ticks.x = element_blank())\nggplotly(plot)\n\n\n\n\n\n\nBeer constitutes the largest volume by far (no surprise), although has been shrinking, with growth in Refreshment Beverage volume. Beer went from 295M litres in 2016 to 268M in 2022, while Refreshment Beverages went from 43M to 86M during the same period.\nPercentage breakdown highlights even further the decline in beer share of volume (still well over 50%), mostly due to increase in Refreshment Beverages sales.\n\n\nCode\n# annual litres by type (stack)\nch_title &lt;- \"Volume (Litres) by Type\"\n# order type by netsales for chart\ntrend_yr_cat$type &lt;- fct_reorder(trend_yr_cat$type, trend_yr_cat$litres)\n\nplot &lt;- trend_yr_cat %&gt;% ggplot(aes(x=year, y=pc_ttl_litres, fill=type))+\n  geom_col(position='fill')+\n  scale_y_continuous(labels=percent_format(), expand=expansion(mult = c(0,0.1)))+\n  scale_fill_manual(values=type_color)+\n  labs(title=ch_title, x=\"\",y=\"\", fill=\"\")+\n  theme(axis.ticks.x = element_blank())\nggplotly(plot)\n\n\n\n\n\n\nBeer has gone from almost 70% to under 60% of volume by type, with Refreshment Beverages taking the share and rising to 19% of total.\n\nBy Type: Year-over-Year % Change in $ Sales\nRefreshment Beverage growth in litres is lower than growth in dollar sales, but still impressive, peaking at over 30% year-over-year.\n\n\nCode\nch_title &lt;- \"% Chg in Volume (Litres), Year-over-Year by Type\"\n## using prev calc. value for first yr, to remove first yr since no % chg value\nplot &lt;- trend_yr_cat %&gt;% filter(year!=min_yr) %&gt;%\n  ggplot(aes(x=year, y=pc_chg_litre))+\n  geom_col(fill=bar_col)+\n  geom_hline(yintercept = 0)+\n  facet_grid(.~type)+\n  scale_y_continuous(labels=percent_format())+\n  theme(strip.background = element_rect(fill = bar_col)) +\n  theme(strip.text=element_text(color='white'))+\n  labs(title=ch_title, x=\"\",y=\"\")+\n  theme(axis.text.x=element_text(hjust=0, vjust=0.5, angle=90),\n        axis.ticks.x = element_blank()\n        ,panel.border = element_rect(fill=NA)\n        ) \nggplotly(plot)\n\n\n\n\n\n\nBeer had declining volume in the most recent 5 years, although may be stabilizing. Other types all had strongest volume growth in 2020 during the pandemic. Refreshment beverages saw decline in growth rate in 2022 after 5 years of strong growth.\nInterestingly, Beer was the only beverage type that did not experience growth during the first year of covid-19 pandemic - maybe due to relatively high consumption of beer consumed in bars and restaurants? That could explain why declining growth decreased as pandemic restrictions faded.\n\n\n\nBy Type: $/Litre\nBreaking out $/litre (proxy for price) by beverage type shows that not all increase at the same rate, and even $/litre decreases are possible.\n\n\nCode\nch_title &lt;- \"$/Litre by Type\"\nplot &lt;- trend_yr_cat %&gt;% ggplot(aes(x=year, y=dollar_per_litre, color=type, group=type))+\n  geom_line(size=1)+\n  geom_point(aes(y=dollar_per_litre), size=2)+\n  scale_y_continuous(labels=label_comma(prefix=\"$\"))+\n  scale_x_discrete(breaks=trend_yr_cat$year)+\n  scale_color_manual(values=type_color)+\n  labs(title=ch_title, x=\"\", y=\"\", color=\"\")\nggplotly(plot)\n\n\n\n\n\n\n\nBy Type: Year-over-Year % Change in $/Litre\nWide range of price categories obscures the changes at high level - % change year-over-year makes trends more visible.\nRefreshment Beverage growth in both $ sales and litre volume comes through in strongest growth in $/litre of any beverage type:\n\n\nCode\nch_title &lt;- \"% Chg in $/Litres, Year-over-Year by Type\"\nplot &lt;- trend_yr_cat %&gt;% filter(year!=min_yr) %&gt;%\n  ggplot(aes(x=year, y=pc_chg_dollar_per_l))+\n  geom_col(fill=bar_col)+\n  geom_hline(yintercept = 0)+\n  facet_grid(.~type)+\n  scale_y_continuous(labels=percent_format())+\n  theme(strip.background = element_rect(fill = bar_col)) +\n  theme(strip.text=element_text(color='white'))+\n  labs(title=ch_title, x=\"\",y=\"\")+\n  theme(axis.text.x=element_text(hjust=0, vjust=0.5, angle=90),\n        axis.ticks.x = element_blank()\n        ,panel.border = element_rect(fill=NA)\n        ) \nggplotly(plot)\n\n\n\n\n\n\nWine is an interesting case of decrease in $/litre in 2020 - under 2.5%, but a decrease nevertheless. It’s possible this may have been influenced by the changes to LDB pricing model for hospitality industry in 2020 during the pandemic, which allowed restaurants and pubs to purchase alcohol at wholesale prices."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html#litre-vs-volume-and-sales",
    "href": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html#litre-vs-volume-and-sales",
    "title": "BC Liquor Sales Analysis: Annual Trends 2016-2022",
    "section": "$/Litre vs Volume and $ Sales",
    "text": "$/Litre vs Volume and $ Sales\nExploring the relationship between changes in $/Litre (proxy for price) and changes in volume, as well as changes in $/Litre and total $ sales:\n\nAre price changes associated with changes in volume purchases?\nAre price changes associated with changes in total $ sales?\n\nNotes on two charts below:\n\ny-axis is adjusted for each beverage type to match the data for that type, so please take that into account.\nred dotted lines are the 0 axis, both vertically and horizontally.\n\n\nChanges in $/Litre vs Changes in Volume\nFor changes in $/litre vs volume, in general, we would expect dots to be in the top left (price decrease, volume increase) or bottom right (price increase, volume decrease).\nIn other words: less is more (and vice versa) - assuming price sensitivity.\n\n\nCode\nch_title &lt;- \"Relationship Between Changes in $/litre and volume\"\n## convert year to numeric for color gradient\ntrend_yr_cat$year &lt;- as.integer(as.character(trend_yr_cat$year)) \n\ntrend_yr_cat %&gt;% ggplot(aes(x=pc_chg_dollar_per_l, y=pc_chg_litre, color=year, label=year))+\n  geom_point()+\n  ## options for labelling - too cluttered\n  #geom_text(vjust=0.2, hjust=-0.2, size=3)+\n  #geom_text_repel(size=3)+\n  scale_color_gradient(low = \"lightblue\", high = \"darkblue\") +\n  facet_grid(type~., scales='free_y')+\n  scale_x_continuous(labels=percent_format())+\n  scale_y_continuous(labels=percent_format())+\n  geom_hline(yintercept=0, linetype='dotted', color='red')+\n  geom_vline(xintercept = 0, linetype='dotted', color='red')+\n  labs(title=ch_title, x=\"% Chg in $/litre\", y=\"% Chg in Litres (scale varies)\")+\n  theme(panel.border = element_rect(fill=NA))\n\n\n\n\n\n\n\n\n\nDifferent types of beverages have different relationships between revenue per litre - our proxy for ‘price’ - and volume sales:\n\nWine is the clearest case in this dataset, where an average price decrease in wine of around 2.5% was associated with ~6% volume growth (top left of Wine panel), price increases around ~1.5%-3% had little apparent effect, and price increases of 6+% were associated with ~3% decline in sales volume.\nBeer looks like a counter-intuitive relationship where the smallest price increases actually tended to be associatedwith largest decreases in volume. Depending on the timing, price decreases may have been in response to softening demand, and may have prevented further decreases in volume. In any case, price changes were relatively modest, and there are probably other factors involved.\nRefreshment beverages don’t appear to have a consistent relationship between price increase and changes in volume during this period of growth, although most recently a ~6% increase in price is associated with a reversal in the growth trend and a small decrease in volume.\n\nClearly, though, there are a variety of factors at work here and not enough data to make any solid conclusions with regard to effect of changes in net sales per litre and litre volumes sold.\n\n\nChanges in $/Litre vs Changes in Volume\nHow does this play out in terms of sales dollars generated along with price and volume changes?\nIf it plays out well for producers/retailers:\n\nUnit price ($/litre) increases may dampen volume sales but result in growth in overall net $ sales. This outcome would land dots in the top right quadrant.\nLikewise, unit price decreases (or smaller increases) will be expected lead to higher volume sales that will more than offset the decrease in unit price and, again, deliver growth in overall net $ sales. Dots would be in the top left.\n\nIf it doesn’t play out as hoped by producers/retailers:\n\nEither price increases will dampen demand so much or price decreases will not lift volume sales high enough, and overall net $ sales decreases. This will result in dots in lower right for the former case, or lower left for the latter.\n\nAdditional notes on chart below:\n\nAs with chart above, y-axis is adjusted for each type, red dotted lines are the 0 axes.\n% chg in volume is indicated by size of the dot. This makes for a busy chart but the intention is to highlight the points that represent the biggest (or smallest) changes in % volume as additional reference.\n\nGood news for the industry is that the positive scenarios held up in almost all cases:\n\n\nCode\nch_title &lt;- \"Relationship Between Changes in $/Litre and Net $ Sales\"\ntrend_yr_cat %&gt;% ggplot(aes(x=pc_chg_dollar_per_l, y=pc_chg_sales, size=pc_chg_litre, color=year, label=year))+\n  geom_point()+\n  guides(size='none')+\n  scale_color_gradient(low = \"lightblue\", high = \"darkblue\") +\n  facet_grid(type~., scales='free_y')+\n  scale_x_continuous(labels=percent_format())+\n  scale_y_continuous(labels=percent_format())+\n  geom_hline(yintercept=0, linetype='dotted', color='red')+\n  geom_vline(xintercept = 0, linetype='dotted', color='red')+\n  labs(title=ch_title, x=\"% Chg in $/litre\", y=\"% Chg in Net $ Sales - scale varies\")+\n  theme(panel.border = element_rect(fill=NA))\n\n\n\n\n\n\n\n\n\nBeer was the only case where changes in $/litre did not always have the hoped for effect of leading to overall higher total net $ sales: the smaller price changes, associated with declines in volume (small dots), resulted in overall decreasing net $ sales. On the plus side, the largest year-over-year increase in $/litre for beer did result in overall increase in net $ sales, even though associated with decrease in litre volume."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html#part-1-wrap-up-next-up",
    "href": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html#part-1-wrap-up-next-up",
    "title": "BC Liquor Sales Analysis: Annual Trends 2016-2022",
    "section": "Part 1 Wrap-up & Next-up",
    "text": "Part 1 Wrap-up & Next-up\nThat concludes our look at annual trends in the BC Liquor Market Review data.\n\nNext-up:\n\nQuarterly patterns: exploration of the seasonal patterns in the BC liquor market by delving into data at the quarterly level, again going back to 2015.\nCategory trends and patterns: closer look at each of the major beverage types, exploring categories and sub-categories within them, as reported in the Liquor Market Review.\n\nComing soon!"
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html#footnotes",
    "href": "posts/bc-liquor-market-review-2023-06-pt01-annual-trends/index.html#footnotes",
    "title": "BC Liquor Sales Analysis: Annual Trends 2016-2022",
    "section": "Footnotes",
    "text": "Footnotes\nNotes on ‘net $ sales’ and ‘$ per litre’:\n\nthe report says “Net dollar value is based on the price paid by the customer and excludes any applicable taxes.”\ncalculating average net dollar value per litre for beverage categories gives unrealistically low numbers compared to retail prices in BC liquor stores. (Beer at average $4/litre? Not even the cheapest beer on the BC Liquor Stores website.)\nthere is likely additional factors related to BC LDB pricing structure, wholesaling, etc.\nbest to consider average net dollar value per litre referred to above as relative indicator."
  },
  {
    "objectID": "posts/google-trends-r-gtrends-for-power-analytics/index.html",
    "href": "posts/google-trends-r-gtrends-for-power-analytics/index.html",
    "title": "Google Trends + R: gtrendsR for Powerful Trend Analytics",
    "section": "",
    "text": "Google Trends is a popular tool for all manner of curiousity related to trends in search activity on the Google search engine:\nAnd lots more.\nGoogle Trends has recently passed its 15th birthday, prompting a Google blog post on “15 Tips for Getting the Most Out of Google Trends”. One thing they noted right at the start is that they used Google Trends to identify search queries related to Google Trends in order to prioritize content for a blog post. This is a classic use case - using Google Trends to fuel content decisions for your marketing.\nAn important point is that Google Trends does not represent or translate to an actual number of searches. This search interest presented is indexed between 0 and 100, where 100 indicates the peak search interest during the particular date range reported on. Everything is relative in Google Trends."
  },
  {
    "objectID": "posts/google-trends-r-gtrends-for-power-analytics/index.html#going-beyond-the-interface",
    "href": "posts/google-trends-r-gtrends-for-power-analytics/index.html#going-beyond-the-interface",
    "title": "Google Trends + R: gtrendsR for Powerful Trend Analytics",
    "section": "Going Beyond the Interface",
    "text": "Going Beyond the Interface\n\n\n\nGoogle Trends Interface\n\n\n(Google Trends example: this query )\nGoogle Trends is a convenient, intuitive interface packed with info that can be great for playing around with, but comes with limitations for tracking trends over time, sharing with others, analyzing more deeply. You can export to a spreadsheet, but this comes with inefficiencies as well: you have to export data for each component in the interface separately and you may lose source information. And if you want to make any adjustments (change date range, geography, category, add terms) you have to go back in there and repeat the process.\nUsing R to work with Google Trends can provide a more efficient solution if you want to:\n\nquickly import Google Trends data into R for further analysis.\ngrab all the modules from the Google Trends interface at once (interest over time, geo data, related topics, related queries).\nrepeat Google Trends reporting to monitor trends over time.\nreproduce the same Google Trends data in future, based on detailed record of your query.\nintegrate Google Trends reporting with other datasets or reporting structures."
  },
  {
    "objectID": "posts/google-trends-r-gtrends-for-power-analytics/index.html#google-trends-in-r",
    "href": "posts/google-trends-r-gtrends-for-power-analytics/index.html#google-trends-in-r",
    "title": "Google Trends + R: gtrendsR for Powerful Trend Analytics",
    "section": "Google Trends in R",
    "text": "Google Trends in R\nGoogle does not provide an official API for Google Trends but the gtrendsR R package created by Philippe Massicotte is a major helper in the accessing Google Trends data within R for reporting and analysis. Along with all the benefits of using R to process and analyze data, the gtrendsR package provides some big advantages over using the Google Trends Interface:\n\nDurability: don’t have to go to the interface and fetch the data each time, you have an on-going reference with source info. You have code that can be referred to, re-used, and shared with others.\nScalability: can expand on existing queries, going beyond the limit of 5 that are available in the tool."
  },
  {
    "objectID": "posts/google-trends-r-gtrends-for-power-analytics/index.html#google-trends-parameters",
    "href": "posts/google-trends-r-gtrends-for-power-analytics/index.html#google-trends-parameters",
    "title": "Google Trends + R: gtrendsR for Powerful Trend Analytics",
    "section": "Google Trends Parameters",
    "text": "Google Trends Parameters\nGoogle Trends has a number of parameters that can be used to fine-tune your search: date ranges, geo data, categories, Google properties. These are available via the gtrendsR package, corresponding to the options in the Google Trend online interface. You just have to know how to tap into them:\n\nDates:\n\n“now 1-H”: last hour - by MINUTE\n“now 4-H”: last 4 hrs - by MINUTE\n“now 1-d”: last day - every 8 MINUTES\n“now 7-d”: last 7 days - HOURLY data\n“today 1-m”: last 30 days - DAILY data\n“today 3-m”: last 90 days - DAILY data\n“today 12-m”: last 12 months - WEEKLY data\n“today+5-y”: last 5 yrs (default) - WEEKLY data\n“all” since beginning of Google Trends 2004\n“YYYY-MM-DD YYYY-MM-DD”: custom start / end date - granularity will depend on time spans above\n\nGeo:\n\nuse gtrendsR::countries to see complete list\nclose to 110,000 options, including country / state / city\ncode below shows how to filter for countries\ngeo=““ for all countries\n\nCategories:\n\nuse gtrendsR::categories\nover 1,400 categories, with ids that are used in the query\ncategory = 0 for all categories\n\nGoogle properties:\n\nspecify one or more of ‘web’, ‘news’, ‘images’, ‘froogle’, ‘youtube’\ngprop=c(“web”, “youtube”) as example for web and youtube search"
  },
  {
    "objectID": "posts/google-trends-r-gtrends-for-power-analytics/index.html#setup---libraries",
    "href": "posts/google-trends-r-gtrends-for-power-analytics/index.html#setup---libraries",
    "title": "Google Trends + R: gtrendsR for Powerful Trend Analytics",
    "section": "Setup - Libraries",
    "text": "Setup - Libraries\nThere’s basically no setup required - no credentials, etc. Only need to load the gtrendsR package. (I’ve pre-loaded other packages I’m using for general purpose, such as ‘tidyverse’, etc.)\n\nlibrary(gtrendsR) ## package for accessing Google Trends - all you need to get going!"
  },
  {
    "objectID": "posts/google-trends-r-gtrends-for-power-analytics/index.html#single-term-query",
    "href": "posts/google-trends-r-gtrends-for-power-analytics/index.html#single-term-query",
    "title": "Google Trends + R: gtrendsR for Powerful Trend Analytics",
    "section": "Single term query",
    "text": "Single term query\nUsing the gtrendsR package to get Google Trends for a single search term.\n\n## basic search\ngt_results &lt;- gtrends(keyword='cryptocurrency',\n        geo=\"\",\n        time=\"now 7-d\",\n        gprop=c(\"web\"),\n        category=0)\n\nThe query returns a bundle of 7 data frames with different info, reflecting what is shown in the Google Trends interface:\n\nnames(gt_results)\n\n[1] \"interest_over_time\"  \"interest_by_country\" \"interest_by_region\" \n[4] \"interest_by_dma\"     \"interest_by_city\"    \"related_topics\"     \n[7] \"related_queries\"    \n\n\n(see screenshot of Google Trends interface for comparison)\n\nInterest over time\nThe ‘interest_over_time’ data frame is the main data object, with relative search volume for the selected search term, country, period, property, and category.\n\nchart_title &lt;- \"Searches for: cryptocurrency\"\nsub_title &lt;- \"Period: past 7 days; Geo: world; Prop: 'web'; Category: all\"\n\n## create chart based on search interest over time\ngt_results$interest_over_time %&gt;% ggplot(aes(x=date, y=hits, color=keyword))+geom_line()+\n  labs(title=chart_title, subtitle=sub_title, x=\"\", y=\"\")\n\n\n\n\n\n\n\n\n\n\nRelated Topics\nThe ‘related_topics’ data frame holds data on queries related to the main search term (‘cryptocurrency’ in this case).\n\nstr(gt_results$related_topics)\n\n'data.frame':   32 obs. of  5 variables:\n $ subject       : chr  \"100\" \"58\" \"58\" \"40\" ...\n $ related_topics: chr  \"top\" \"top\" \"top\" \"top\" ...\n $ value         : chr  \"Bitcoin\" \"Cryptocurrency exchange\" \"Investment\" \"Currency\" ...\n $ keyword       : chr  \"cryptocurrency\" \"cryptocurrency\" \"cryptocurrency\" \"cryptocurrency\" ...\n $ category      : int  0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"reshapeLong\")=List of 4\n  ..$ varying:List of 1\n  .. ..$ value: chr \"top\"\n  .. ..- attr(*, \"v.names\")= chr \"value\"\n  .. ..- attr(*, \"times\")= chr \"top\"\n  ..$ v.names: chr \"value\"\n  ..$ idvar  : chr \"id\"\n  ..$ timevar: chr \"related_topics\"\n\n\n\nsubject: relative value to main search term\nrelated_topics: contains ‘top’ topics and ‘rising’ topics\nvalue: related topic\nkeyword: main search term\ncategory: search term category, if applicable\n\n\n  chart_title &lt;- \"crytopcurrency: related topics\"\n  ## \n  top &lt;- gt_results$related_topics %&gt;% filter(related_topics=='top' & !is.na(subject) &\n                                                subject!='&lt;1')\n  ## convert value to factor and subject to numeric\n  top$value &lt;- as.factor(top$value)\n  top$subject &lt;- as.numeric(top$subject)\n  ## PLOT related topics\n  top %&gt;% ggplot(aes(x=reorder(value, subject), y=subject))+geom_col()+\n  coord_flip()+\n    scale_y_continuous(expand=expansion(add=c(0,10)))+\n    labs(title=chart_title, y='', x='')\n\n\n\n\n\n\n\n\n\n\nRelated Queries\nRelated Queries module has similar structure to Related Topics:\n\nstr(gt_results$related_queries)\n\n'data.frame':   50 obs. of  5 variables:\n $ subject        : chr  \"100\" \"74\" \"69\" \"67\" ...\n $ related_queries: chr  \"top\" \"top\" \"top\" \"top\" ...\n $ value          : chr  \"crypto\" \"cryptocurrency price\" \"what is cryptocurrency\" \"bitcoin cryptocurrency\" ...\n $ keyword        : chr  \"cryptocurrency\" \"cryptocurrency\" \"cryptocurrency\" \"cryptocurrency\" ...\n $ category       : int  0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"reshapeLong\")=List of 4\n  ..$ varying:List of 1\n  .. ..$ value: chr \"top\"\n  .. ..- attr(*, \"v.names\")= chr \"value\"\n  .. ..- attr(*, \"times\")= chr \"top\"\n  ..$ v.names: chr \"value\"\n  ..$ idvar  : chr \"id\"\n  ..$ timevar: chr \"related_queries\"\n\n\n\n  chart_title &lt;- \"crytopcurrency: related queries\"\n  ## \n  top &lt;- gt_results$related_queries %&gt;% filter(related_queries=='top' & !is.na(subject) &\n                                                subject!='&lt;1')\n  ## convert value to factor and subject to numeric\n  top$value &lt;- as.factor(top$value)\n  top$subject &lt;- as.numeric(top$subject)\n  ## PLOT related topics\n  top %&gt;% ggplot(aes(x=reorder(value, subject), y=subject))+geom_col()+\n  coord_flip()+\n    scale_y_continuous(expand=expansion(add=c(0,10)))+\n    labs(title=chart_title, y='', x='')\n\n\n\n\n\n\n\n\nYou can see right there the possibilities for content marketing: if you’re a crypto currency blogger, for example, you may want to write about how to identify the ‘best cryptocurrency’, etc."
  },
  {
    "objectID": "posts/google-trends-r-gtrends-for-power-analytics/index.html#multi-term-query",
    "href": "posts/google-trends-r-gtrends-for-power-analytics/index.html#multi-term-query",
    "title": "Google Trends + R: gtrendsR for Powerful Trend Analytics",
    "section": "Multi-Term Query",
    "text": "Multi-Term Query\nThe same approach used to query for single terms can be extended to multiple terms. The example below shows how to load up a collection of terms, as well as leveraging other variables for the query.\n\n## create list of multiple search terms\nsrch_term &lt;- c(\"cryptocurrency\",\n               \"bitcoin\",\n               \"ethereum\",\n               \"stock market\",\n               \"real estate\")\nperiod &lt;- \"today 12-m\"\nctry &lt;- \"\" ## blank = world; based on world countries ISO code\nprop &lt;- c(\"web\")\ncat &lt;- 0 ## 0 = all categories\n\n## user-friendly versions of parameters for use in chart titles or other query descriptions\nctry_ &lt;- ifelse(ctry==\"\",\"world\",ctry)\nprop_ &lt;- paste0(prop, collapse=\", \")\ncat_ &lt;- ifelse(cat==0,\"all\",cat)\n\n## use gtrendsR to call google trends API\n# works: c(\"cryptocurrency\",\"bitcoin\",\"ethereum\",\"stock market\",\"real estate\")\ngt_results_multi &lt;- gtrends(keyword=srch_term,\n        geo=ctry,\n        time=period,\n        gprop=prop,\n        category=cat)\n## confirm keywords\n#table(gt_results$interest_over_time$keyword)\n\nThe gt_results object returned is the same as with single query, just has more values for the ‘keyword’ variable in each data frame.\n\nInterest over time\n\nchart_title &lt;- paste0(\"Search trends: \", paste(srch_term[1:2], collapse=\", \"), \" +\")\nsub_title &lt;- paste0(\"Period: \", period, \"; Geo: \", ctry_, \"; Prop: \", prop_, \"; Category: \", cat_)\n\n## create chart based on search interest over time\nmulti &lt;- gt_results_multi$interest_over_time %&gt;% filter(hits!=0) %&gt;% ggplot(aes(x=date, y=hits, color=keyword))+geom_line()+\n  scale_y_continuous(expand=expansion(add=c(0,0)))+\n  labs(title=chart_title, subtitle=sub_title, x=\"\", y=\"\")+\n  theme(legend.position = \"top\")\nggplotly(multi)\n\n\n\nPeriod: today 12-m (2023-04-16); Geo: world; Prop: web; Category: all\n\n\nVery similar results to chart shown at the top. Average search index for these keywords over this period also the same as shown in Google Trends interface.\n\ngt_results_multi$interest_over_time %&gt;% filter(hits!=0) %&gt;% group_by(keyword) %&gt;%\n  summarize(avg=round(mean(hits))) %&gt;% \n  ggplot(aes(x=keyword, y=avg, label=avg, fill=keyword))+\n  geom_col()+\n  #geom_col(fill='navyblue')+\n  geom_hline(yintercept = 0)+\n  geom_text(size=6, nudge_y = 3)+\n  labs(title=\"Average Search by Keyword\", x=\"\",y=\"\")+\n  theme(axis.text.y = element_blank(),\n        axis.text.x = element_text(size=12),\n        panel.grid = element_blank(),\n        legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nAs noted, there is the same limitation as the interface of maximum five terms at a time. Of course, one of the beauties of doing thing programmatically is that provides opportunities for combining queries to go beyond five. For that, you need to make sure there is a common term in each queries to calculate the relative values. This is a topic for another blog post."
  },
  {
    "objectID": "posts/google-trends-r-gtrends-for-power-analytics/index.html#search-terms-vs-search-topics",
    "href": "posts/google-trends-r-gtrends-for-power-analytics/index.html#search-terms-vs-search-topics",
    "title": "Google Trends + R: gtrendsR for Powerful Trend Analytics",
    "section": "Search Terms vs Search ‘Topics’",
    "text": "Search Terms vs Search ‘Topics’\nTip #3 in “15 Tips for Getting the Most Out of Google Trends” mentions the importance of choosing search ‘topics’ when available for a given search term.\nUsing a topic version of the term has benefits, but also complications for programmatic access:\n\nyou need to go to Google Trends to check if a topic is available (it may also be called something different than ‘topic’, like ‘currency’ for the term ‘Bitcoin’).\nthe topic term is an indecipherable code, as circled in the URL in the browser bar above.\ncomparisons may be skewed if mixing terms and topics.\n\n\n\n\nGoogle Trends search type example - full"
  },
  {
    "objectID": "posts/r-time-series-vs-data-frame/index.html",
    "href": "posts/r-time-series-vs-data-frame/index.html",
    "title": "R Time Series Objects vs Data Frames: Advantages and Limitations",
    "section": "",
    "text": "As I learn more and more about R, questions often arise about which packages/methods/tools to use for a given situation. R is a vast - and growing - universe and I’m not interested in learning everything in that universe. I’m interested in learning the shortest paths between where I am now and my objective. As an adherent of the tidyverse, I lean strongly toward solutions in that realm. But, to paraphrase an old saying, ‘tidyverse is a playgound…not a jail’ and if a problem can be handled better by stepping outside the tidyverse, I’m all for that.\nOne of these areas is in dealing with time series: data sets comprised of repeated measurements over consistent time intervals (hourly, daily, monthly, etc). You can work with time series data using data frames, the fundamental building block of data analysis in R, but there are more specialized tools that offer more flexibility, specific capabilities and ease of use when analyzing time-based data. This can come into play in a wide variety of situations: weekly website visits, monthly sales, daily stock prices, annual GDP, electricity use by minute, that kind of thing.\nSo what are these time series advantages? How do we leverage them? What limitations of time series objects are good to be aware of? I’m not pretending this is a definitive guide, but I’ve been looking at this for a while and hear are my observations…\n(A word on forecasting: this is a MAJOR use case for time series but is not the main focus here and I’ll only touch on that briefly below.)\n\nTime Series Essentials\nts is the basic class for time series objects in R. You can do a lot with ts but its functionality have been extend by other packages, in particular zoo and more recently xts.\nxts is a leading, evolved R package for working with time series. It builds on zoo, an earlier pkg for handling time series in R. Datacamp has a very nice\nSo I’m just going to scratch the surface and hit some highlights with examples here.\n\nGet a Time Series Object\nAt its most basic, a time series object is a list or sometimes matrix of observations at regular time intervals.\nExamples in built-in R data sets include:\n\nannual Nile river flows\n\n\nclass(Nile)\n\n[1] \"ts\"\n\nstr(Nile)\n\n Time-Series [1:100] from 1871 to 1970: 1120 1160 963 1210 1160 1160 813 1230 1370 1140 ...\n\nNile\n\nTime Series:\nStart = 1871 \nEnd = 1970 \nFrequency = 1 \n  [1] 1120 1160  963 1210 1160 1160  813 1230 1370 1140  995  935 1110  994 1020\n [16]  960 1180  799  958 1140 1100 1210 1150 1250 1260 1220 1030 1100  774  840\n [31]  874  694  940  833  701  916  692 1020 1050  969  831  726  456  824  702\n [46] 1120 1100  832  764  821  768  845  864  862  698  845  744  796 1040  759\n [61]  781  865  845  944  984  897  822 1010  771  676  649  846  812  742  801\n [76] 1040  860  874  848  890  744  749  838 1050  918  986  797  923  975  815\n [91] 1020  906  901 1170  912  746  919  718  714  740\n\n\n\nmonthly Air Passengers - yes, I know everybody uses Air Passengers for their time series example. So damn handy. Different examples below, I promise. ;)\n\n\nclass(AirPassengers)\n\n[1] \"ts\"\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nBoth these examples are time series of the ts class, and we can see right off that these are different data structures from data frames. A key thing to note about time series is that date/time is not in a column the way it would be in a data frame, but is in an index - similar to row.names in a data frame.\nIf we look at the index for the Nile river data, we can see the time values and we can check the start and end. This info corresponds to the structure info shown above, where start = 1871, end = 1970, and frequency = 1, meaning 1 observation per year, annual data.\n\nindex(Nile)\n\n  [1] 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885\n [16] 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900\n [31] 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915\n [46] 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930\n [61] 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945\n [76] 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960\n [91] 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n\nstart(Nile)\n\n[1] 1871    1\n\nend(Nile)\n\n[1] 1970    1\n\n\nAs discussed above, ts is useful, but xts offers additional flexibility and features.\n\n\nConvert ts to xts\nConverting to an xts object can often make the data more intuitive to deal with.\n\nlibrary(xts)\nNile_xts &lt;- as.xts(Nile)\nstr(Nile_xts)\n\nAn xts object on 1871-01-01 / 1970-01-01 containing: \n  Data:    double [100, 1]\n  Index:   Date [100] (TZ: \"UTC\")\n\nhead(Nile_xts)\n\n           [,1]\n1871-01-01 1120\n1872-01-01 1160\n1873-01-01  963\n1874-01-01 1210\n1875-01-01 1160\n1876-01-01 1160\n\n\n\nAir_xts &lt;- as.xts(AirPassengers)\nstr(Air_xts)\n\nAn xts object on Jan 1949 / Dec 1960 containing: \n  Data:    double [144, 1]\n  Index:   yearmon [144] (TZ: \"UTC\")\n\nhead(Air_xts)\n\n         [,1]\nJan 1949  112\nFeb 1949  118\nMar 1949  132\nApr 1949  129\nMay 1949  121\nJun 1949  135\n\n\n\nWe can see here that xts has reshaped the data from a matrix with rows by year and columns by month to more ‘tidy’ data with mth-year as index and observations in one column.\n\n\n\nNative xts\nSome data comes as xts time series out of the box. For example, the quantmod package fetches stock market data as xts time series automatically:\n\nlibrary(quantmod)\n## use quantmod pkg to get some stock prices as time series\nprice &lt;- getSymbols(Symbols='EA', from=\"2020-01-01\", to=Sys.Date(), auto.assign=FALSE)\nclass(price)\n\n[1] \"xts\" \"zoo\"\n\nhead(price)\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2020-01-02     108     108    107      107   1901000         106\n2020-01-03     106     108    105      107   1840300         106\n2020-01-06     107     109    107      109   2934200         107\n2020-01-07     109     109    108      108   1692400         107\n2020-01-08     108     110    108      109   2651600         108\n2020-01-09     110     110    108      109   1818600         108\n\n\nAs noted, a key characteristic of time series object is that dates are in an index rather than being in a date column, as they would be in typical data frame. Looking at the structure of the xts object, we can again see it is different from a data frame.\n\nstr(price)\n\nAn xts object on 2020-01-02 / 2023-04-06 containing: \n  Data:    double [822, 6]\n  Columns: EA.Open, EA.High, EA.Low, EA.Close, EA.Volume ... with 1 more column\n  Index:   Date [822] (TZ: \"UTC\")\n  xts Attributes:\n    $ src    : chr \"yahoo\"\n    $ updated: POSIXct[1:1], format: \"2023-04-08 16:50:35\"\n\n\n\n\nConvert xts to data frame\nIf you want to work with the time series as a data frame, it is fairly straightforward to convert an xts object:\n\nprice_df &lt;- as.data.frame(price)\n## add Date field based on index (row names) of xts object\nprice_df$Date &lt;- index(price)\n## set data frame row names to numbers instead of dates\nrownames(price_df) &lt;- seq(1:nrow(price))\n## reorder columns to put Date first\nprice_df &lt;- price_df %&gt;% select(Date, 1:ncol(price_df)-1)\n## check out structure using glimpse, as is the fashion of the times\nglimpse(price_df)\n\nRows: 822\nColumns: 7\n$ Date        &lt;date&gt; 2020-01-02, 2020-01-03, 2020-01-06, 2020-01-07, 2020-01-0…\n$ EA.Open     &lt;dbl&gt; 108, 106, 107, 109, 108, 110, 109, 109, 110, 110, 110, 112…\n$ EA.High     &lt;dbl&gt; 108, 108, 109, 109, 110, 110, 109, 110, 110, 110, 111, 113…\n$ EA.Low      &lt;dbl&gt; 107, 105, 107, 108, 108, 108, 108, 109, 109, 109, 110, 112…\n$ EA.Close    &lt;dbl&gt; 107, 107, 109, 108, 109, 109, 109, 110, 110, 110, 111, 113…\n$ EA.Volume   &lt;dbl&gt; 1901000, 1840300, 2934200, 1692400, 2651600, 1818600, 1756…\n$ EA.Adjusted &lt;dbl&gt; 106, 106, 107, 107, 108, 108, 107, 108, 108, 108, 110, 111…\n\n\nData frame is basically a straight-up table, whereas the xts object has other structural features.\n\n\nConvert data frame to xts\n\n## convert data frame to xts object by specifying the date field to use for xts index.\nprice_xts &lt;- xts(price_df, order.by=as.Date(price_df$Date))\nstr(price_xts)\n\nAn xts object on 2020-01-02 / 2023-04-06 containing: \n  Data:    character [822, 7]\n  Columns: Date, EA.Open, EA.High, EA.Low, EA.Close ... with 2 more columns\n  Index:   Date [822] (TZ: \"UTC\")\n\n\nNotice, however, that in the process of converting an xts object to data frame and back to xts, the xts Attributes information has been lost.\n\n\nSaving/Exporting time series data\nDue to the structure of an xts object, the best way to save/export for future use in R and preserve all its attributes is to save as RDS file, using saveRDS. (additional helpful RDS info here.)\nHowever, this won’t be helpful if you need to share the data with someone who is not using R. You can save as a CSV file using write.zoo (be sure to specificy sep=“,”) and this will maintain the table structure of the data but will drop the attributes. It will automatically move the indexes into an Index column so if someone opens it in Excel/Google Sheets, they will see the dates/times.\nSaving as RDS or CSV:\n\n## save as RDS to preserve attributes\nsaveRDS(price, file=\"price.rds\")\nprice_rds &lt;- readRDS(file='price.rds')\nstr(price_rds)\n\nAn xts object on 2020-01-02 / 2023-04-06 containing: \n  Data:    double [822, 6]\n  Columns: EA.Open, EA.High, EA.Low, EA.Close, EA.Volume ... with 1 more column\n  Index:   Date [822] (TZ: \"UTC\")\n  xts Attributes:\n    $ src    : chr \"yahoo\"\n    $ updated: POSIXct[1:1], format: \"2023-04-08 16:50:35\"\n\n## save as CSV - ensure to include sep=\",\"\nwrite.zoo(price, file='price.csv', sep=\",\")\nprice_zoo &lt;- read_csv('price.csv')\n\n\n\n\nTime Series Strengths\nThe structure of a time series leads a variety of advantages related to time-based analysis, compared to data frames. A few of the main ones, at least from my perspective:\n\nPeriod/Frequency Manipulation: can easily change from granular periods, such as daily, to aggregated periods.\nPeriod calculations: counting number of periods in the data (months, quarters, years).\nSelection/subsetting based on date ranges.\nVisualization: a number of visualization options are designed to work with time series.\nDecomposition: breaking out time series into trend, seasonal, random components for analysis.\nForecasting: time series objects are designed for applying various forecasting methods like Holt-Winters and ARIMA. This is well beyond the scope of this post, but we’ll show a quick ARIMA example.\n\nNo doubt everything you can do with time series can be done with data frames, but using a time series object can really expedite things.\n\n\nTime Series Manipulation/Calculation\n\nPeriod/Frequency Manipulation\nChange the period granularity to less granular:\n\neasily change daily data to weekly, monthly, quarterly, yearly\n\n\n## get periodicity (frequency) for data set\nperiodicity(price)\n\nDaily periodicity from 2020-01-02 to 2023-04-06 \n\n## aggregate by period\nhead(to.weekly(price)[,1:5])\n\n           price.Open price.High price.Low price.Close price.Volume\n2020-01-03        108        108       105         107      3741300\n2020-01-10        107        110       107         109     10852800\n2020-01-17        109        113       109         113     10221500\n2020-01-24        113        114       112         112      8457000\n2020-01-31        110        113       106         108     18435600\n2020-02-07        108        111       104         109     15973600\n\nhead(to.monthly(price)[,1:5])\n\n         price.Open price.High price.Low price.Close price.Volume\nJan 2020      107.9        114     105.1         108     51708200\nFeb 2020      107.9        111      98.6         101     55140000\nMar 2020      101.9        112      85.7         100    116497800\nApr 2020       98.4        119      96.7         114     72981400\nMay 2020      113.1        123     111.1         123     71400200\nJun 2020      123.3        134     113.3         132     64143400\n\nhead(to.yearly(price)[,1:5])\n\n           price.Open price.High price.Low price.Close price.Volume\n2020-12-31        108        147      85.7         144    747474600\n2021-12-31        143        150     120.1         132    641690700\n2022-12-30        132        143     109.2         122    547899700\n2023-04-06        124        131     108.5         125    167447300\n\n\nNotice that this isn’t a straight roll-up but actual summary: for the monthly data, the High is max of daily data for the month, the Low is minimum for the month, while volume is the sum for the month, all as you would expect.\nYou can also pull out the values at the END of a period-length, including setting number of periods to skip over each iteration:\n\nget index for last day of period length specified in ‘on’ for every k period.\napply index to dataset to extract the rows.\n\n\n## every 2 weeks (on='week's, k=2)\nend_wk &lt;- endpoints(price, on=\"weeks\", k=2)\nhead(price[end_wk,])\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2020-01-03   105.6   107.8  105.1    107.2   1840300       105.7\n2020-01-17   112.4   113.0  111.6    112.9   3053300       111.4\n2020-01-31   110.8   110.8  105.5    107.9   6995800       106.5\n2020-02-14   108.9   109.9  108.8    109.7   1227500       108.2\n2020-02-28   100.3   101.9   98.6    101.4   6853700       100.0\n2020-03-13    98.7    99.6   92.8     97.1   5842000        95.7\n\n## every 6 months\nend_mth &lt;- endpoints(price, on='months', k=6)\nhead(price[end_mth,])\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2020-06-30     133     133    131      132   2177400         130\n2020-12-31     142     144    142      144   1689900         142\n2021-06-30     144     145    143      144   1799900         142\n2021-12-31     134     135    132      132   1610900         131\n2022-06-30     122     123    121      122   2319000         121\n2022-12-30     122     122    121      122   1164400         122\n\n\nSee end of Period Calculations section for how to get an average during periods shown: averages for each 6 month period, for example.\n\n\nPeriod Counts\n\n## get the number of weeks, months, years in the dataset (including partial)\nprice_nw &lt;- nweeks(price)\nprice_nm &lt;- nmonths(price)\nprice_ny &lt;- nyears(price)\n\nThe price data covers:\n\n822 days\n171 weeks\n40 months\n4 years (or portions thereof)\n\nFirst/last dates:\n\n## get earliest date\nst_date &lt;- start(price)\n## get last date\nend_date &lt;- end(price)\n\n\nStart: 2020-01-02\nEnd: 2023-04-06\n\n\n\nSelecting/Subsetting\nTime series objects make it easy to slice the data by date ranges. This is an area where time series really shine compared to trying to do the same thing with a data frame.\n\nxts is super-efficient at interpreting date ranges based on minimal info.\n‘/’ is a key symbol for separating dates - it is your friend.\ndate ranges are inclusive of references used.\n\nNote that in the following examples based on stock market data, dates are missing due to gaps in data - days when markets closed.\n\nquickly get entire YEAR\n\n\n## subset on a YEAR (showing head and tail to confirm data is 2021 only)\nhead(price[\"2021\"])\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2021-01-04     143     144    138      140   3587000         138\n2021-01-05     140     141    138      141   2117800         140\n2021-01-06     139     140    136      137   2398500         135\n2021-01-07     137     141    137      141   2936200         139\n2021-01-08     141     142    140      142   1902700         140\n2021-01-11     142     142    139      141   2589800         139\n\ntail(price[\"2021\"])\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2021-12-23     131     133    131      133   1594000         132\n2021-12-27     133     134    132      133   1377300         132\n2021-12-28     133     135    133      133   1230700         132\n2021-12-29     134     134    132      133    912300         132\n2021-12-30     134     136    134      134   1177000         133\n2021-12-31     134     135    132      132   1610900         131\n\n\n\nDURING selected month\n\n\n## get data DURING selected month\nprice[\"2020-02\"]\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2020-02-03     108     109  104.4      105   4155500         104\n2020-02-04     106     107  105.3      107   4190500         106\n2020-02-05     109     109  107.1      108   2895700         106\n2020-02-06     109     110  108.4      110   2500600         109\n2020-02-07     109     111  108.7      109   2231300         108\n2020-02-10     109     110  108.2      109   2170800         107\n2020-02-11     109     109  107.9      109   1195300         108\n2020-02-12     110     110  108.6      110   1567200         108\n2020-02-13     109     109  108.0      109   1627000         107\n2020-02-14     109     110  108.8      110   1227500         108\n2020-02-18     109     110  108.7      109   2171400         108\n2020-02-19     110     111  109.4      110   1540000         108\n2020-02-20     109     109  107.4      109   4034000         108\n2020-02-21     108     109  106.8      108   2546900         107\n2020-02-24     105     108  105.0      107   2817100         106\n2020-02-25     108     109  105.2      105   3651300         104\n2020-02-26     106     108  105.6      107   2858000         105\n2020-02-27     104     106  102.7      103   4906200         101\n2020-02-28     100     102   98.6      101   6853700         100\n\n\n\nFROM start of year to END OF SPECIFIC MONTH\n\n\n## get data FROM start of a year to END OF SPECIFIC MONTH\nprice_jf &lt;- price[\"2021/2021-02\"]\nhead(price_jf, 4)\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2021-01-04     143     144    138      140   3587000         138\n2021-01-05     140     141    138      141   2117800         140\n2021-01-06     139     140    136      137   2398500         135\n2021-01-07     137     141    137      141   2936200         139\n\ntail(price_jf, 3)\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2021-02-24     138     140    137      138   3735500         136\n2021-02-25     137     139    134      135   3042600         134\n2021-02-26     136     138    134      134   3646600         132\n\n\n\neverything BEFORE specified date\n\n\n## get everything BEFORE specified date (based on what is avaliable)\nprice[\"/2020-01-06\"]\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2020-01-02     108     108    107      107   1901000         106\n2020-01-03     106     108    105      107   1840300         106\n2020-01-06     107     109    107      109   2934200         107\n\n\n\neverything BETWEEN two dates\n\n\n## get everything BETWEEN two dates\nprice[\"2021-06-01/2021-06-04\"]\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2021-06-01     142     144    142      144   2610300         142\n2021-06-02     144     144    141      141   1522100         140\n2021-06-03     141     143    141      142   1574900         141\n2021-06-04     143     146    143      145   1919500         144\n\n\n\neverything AFTER specified date\n\n\n## get everything AFTER specified date\nprice[\"2022-01-18/\"]\n\n           EA.Open EA.High  EA.Low EA.Close EA.Volume EA.Adjusted\n2022-01-18     138     143     133      134   8758900         133\n2022-01-19     136     138     135      137   3820300         136\n2022-01-20     138     142     138      139   3116900         138\n2022-01-21     138     141     138      139   3120000         138\n2022-01-24     137     139     132      135   4262100         134\n2022-01-25     134     134     130      131   2386400         130\n2022-01-26     131     132     129      130   2333800         129\n2022-01-27     131     134     131      131   1781500         130\n2022-01-28     131     132     130      132   2155000         131\n2022-01-31     131     136     129      133   4471000         132\n       ...                                                       \n2023-03-24     118     119     118      119   2527300         119\n2023-03-27     119     119     118      119   2276500         119\n2023-03-28     118     118     117      118   1551100         118\n2023-03-29     118     119     118      119   1522800         119\n2023-03-30     120     120     119      119   1979500         119\n2023-03-31     119     121     119      120   2346200         120\n2023-04-03     120     122     120      121   1946400         121\n2023-04-04     121     125     121      125   3303700         125\n2023-04-05     125     126     125      126   2564700         126\n2023-04-06     126     126     125      125   1991000         125\n\n\n\n\nPeriod Calculations\nTime series objects lend themselves well to time-based calculations.\nSimple arithmetic between two dates is not as straightforward as might be expected, but still easily doable:\n\n## subtraction of a given metric between two dates\nas.numeric(price$EA.Close[\"2022-01-21\"])-as.numeric(price$EA.Close[\"2022-01-18\"])\n\n[1] 5.1\n\n## subtraction of one metric from another on same date\nprice$EA.Close[\"2022-01-18\"]-price$EA.Open[\"2022-01-18\"]\n\n           EA.Close\n2022-01-18    -4.53\n\n\nLag.xts is versatile for lag calculations, calculating differences over time:\n\n## calculates across all columns with one command - default is 1 period but can be set with k\nhead(price-lag.xts(price))\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2020-01-02      NA      NA     NA       NA        NA          NA\n2020-01-03   -2.36   -0.60  -1.64    -0.14    -60700      -0.138\n2020-01-06    1.37    1.56   1.51     1.58   1093900       1.559\n2020-01-07    2.05   -0.06   1.10    -0.39  -1241800      -0.385\n2020-01-08   -0.82    0.75   0.05     1.10    959200       1.085\n2020-01-09    1.82    0.34   0.49    -0.13   -833000      -0.128\n\n## set k for longer lag - this example starting at a date beyond available data for the lag calculations, so no NAs\nhead(price[\"2020-01-13/\"]-lag.xts(price, k=7))\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2020-01-13    1.11    1.72   1.86     2.48    -43600       2.446\n2020-01-14    4.08    2.42   3.59     2.38   -116900       2.348\n2020-01-15    2.84    1.14   2.51     0.83  -1455100       0.819\n2020-01-16    1.00    2.04   2.22     2.87    415900       2.831\n2020-01-17    4.19    2.99   3.77     3.44    401700       3.393\n2020-01-21    2.55    2.63   3.50     3.05    346400       3.009\n\n## works for individual column\nprice$EA.Close[\"2022-01-18/\"]-lag.xts(price$EA.Close, k=2)\n\n           EA.Close\n2022-01-18     3.07\n2022-01-19     6.47\n2022-01-20     4.97\n2022-01-21     2.10\n2022-01-24    -3.68\n2022-01-25    -8.00\n2022-01-26    -5.22\n2022-01-27     0.05\n2022-01-28     1.94\n2022-01-31     1.60\n       ...         \n2023-03-24     5.87\n2023-03-27     2.60\n2023-03-28    -1.01\n2023-03-29     0.55\n2023-03-30     1.08\n2023-03-31     1.26\n2023-04-03     2.25\n2023-04-04     4.79\n2023-04-05     4.80\n2023-04-06    -0.08\n\n\nDiff for calculating differences, based on combination of lag and difference order:\n\nhead(diff(price, lag=1, differences=1))\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2020-01-02      NA      NA     NA       NA        NA          NA\n2020-01-03   -2.36   -0.60  -1.64    -0.14    -60700      -0.138\n2020-01-06    1.37    1.56   1.51     1.58   1093900       1.559\n2020-01-07    2.05   -0.06   1.10    -0.39  -1241800      -0.385\n2020-01-08   -0.82    0.75   0.05     1.10    959200       1.085\n2020-01-09    1.82    0.34   0.49    -0.13   -833000      -0.128\n\nhead(diff(price, lag=1, differences=2))\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2020-01-02      NA      NA     NA       NA        NA          NA\n2020-01-03      NA      NA     NA       NA        NA          NA\n2020-01-06    3.73    2.16   3.15     1.72   1154600        1.70\n2020-01-07    0.68   -1.62  -0.41    -1.97  -2335700       -1.94\n2020-01-08   -2.87    0.81  -1.05     1.49   2201000        1.47\n2020-01-09    2.64   -0.41   0.44    -1.23  -1792200       -1.21\n\n\n\nfirst example: diff with lag=1, differences=1 gives same result as lag.xts with k=1 (or default)\nsecond example: diff with differences=2 gives the ‘second order difference’: difference between the differences.\n\nEA.Open:\n\n3.73 = 1.37-(-2.36)\n0.68 = 2.05-1.37\n-2.87 = -0.82-2.05\n…\n\n\n\nUseful for some forecasting methods, among other applications.\nReturns for calculating % change period over period:\n\nfunctions in quantmod package designed for financial asset prices, but can be applied to other xts data.\nvarious periodicity: daily, weekly, monthly, quarterly, yearly or ALL at once (allReturn())\n\n\nhead(dailyReturn(price))\n\n           daily.returns\n2020-01-02      -0.00556\n2020-01-03      -0.00130\n2020-01-06       0.01474\n2020-01-07      -0.00359\n2020-01-08       0.01015\n2020-01-09      -0.00119\n\nhead(monthlyReturn(price))\n\n           monthly.returns\n2020-01-31       -0.000185\n2020-02-28       -0.060693\n2020-03-31       -0.011838\n2020-04-30        0.140661\n2020-05-29        0.075442\n2020-06-30        0.074626\n\n\n\napplied to Air Passenger xts to get % change, even though not financial returns:\n\n\nhead(monthlyReturn(Air_xts))\n\n         monthly.returns\nJan 1949          0.0000\nFeb 1949          0.0536\nMar 1949          0.1186\nApr 1949         -0.0227\nMay 1949         -0.0620\nJun 1949          0.1157\n\n\nAverage for period:\n\nUsing the indexes obtained in the ‘endpoints’ example at the end of the Period/Frequency Manipulation section above, calculate averages for the periods.\n\n\nperiod.apply(price, INDEX=end_mth, FUN=mean)\n\n           EA.Open EA.High EA.Low EA.Close EA.Volume EA.Adjusted\n2020-06-30     111     113    110      112   3454968         110\n2020-12-31     133     134    131      133   2465653         131\n2021-06-30     140     142    139      140   2508342         139\n2021-12-31     138     139    136      137   2583252         136\n2022-06-30     129     131    127      129   2530396         128\n2022-12-30     126     127    125      126   1843548         126\n2023-04-06     118     119    117      118   2537080         118\n\n\nRolling Average:\nYou can also calculate a rolling (moving) average quickly with ‘rollmean’ function from zoo:\n\n## get subset of data for demo\nprice_c &lt;- price[,'EA.Close']\nprice_c &lt;- price_c['/2020-02-28']\n## calc rolling mean and add to original data \n## - k=3 means 3-period lag\n## - align='right' put calculated number at last date in rolling period\nprice_c$EA_CLose_rm &lt;- rollmean(price_c, k=3, align='right')\n\n## quick dygraph - more on this below\ndygraph(price_c, width='100%')\n\n\n\n\n\n\n\n\n\nVisualization\nTime series objects offer some different visualization opportunities than data frames. Below are a couple of options.\n\nPlot.ts\nYou can do a quick, simple plot with plot.ts(). Note that in this case the x-axis is the numerical index of the data point, and doesn’t show the date.\n\nplot.ts(price$EA.Close)\n\n\n\n\n\n\n\n\n\n\nDygraphs\nThe dygraphs package offers flexibility and interactivity for time series.\n\neasily show multiple metrics at once.\nscroll over to see details.\nselect chart area to zoom in.\n\n\nlibrary(dygraphs)\ndygraph(price[,1:4], width='100%')\n\n\n\n\n\n\n\nsubset for individual columns.\neasily add annotations for events.\n\n\n## use dyEvent to add annotations\ngraph &lt;- dygraph(price$EA.Close, width='100%')\ngraph &lt;- dyEvent(graph, \"2020-02-21\",\"Start of Covid 19\", labelLoc = 'top')\ngraph &lt;- dyEvent(graph, \"2021-06-10\",\"New product announcements\", labelLoc = 'top')\n## print chart\ngraph\n\n\n\n\n\n\n\n\n\nDecomposition Plots\nDecomposition of a time series enables you to view it broken out into 3 key components (in addition to observed values):\n\noverall trend\nseasonality trending\nrandomness trend (noise)\n\nThis can make it easier to ‘separate the signal from the noise’ and get a clearer sense of what is going on.\nThere has to be data over a long enough period to assess any seasonal trend, so this requires:\n\nfrequency &gt; 1, where 1=annual data; typically it would be at least 4 (quarterly), 12 (monthly), 52 (weekly), 365 (daily).\nperiod longer than 2 years: one year is not enough to establish a seasonal pattern over time.\n\nif you get ‘Error in decomposet(): time series has no or less than 2 periods’ it is usually due to violating one or both of the above conditions.\n\nneed to translate xts object to ts for this.\n\n\n## Air Passengers has enough data\nap_decomp &lt;- decompose(AirPassengers)\nplot(ap_decomp)\n\n\n\n\n\n\n\napx_decomp &lt;- decompose(ts(Air_xts, frequency=12))\nplot(apx_decomp)\n\n\n\n\n\n\n\n\n\nsame results with both approaches, although the original ts object maintains dates on x-axis, making it easier to interpret.\ninterpretation: steady upward trend; peaks at mid-year; randomness fairly large at first, settles down, then appears to be growing over time.\ncoincides with what we see in the observed data but makes the patterns more evident.\n\nIf we fetch some longer daily data for stock price, we can do the same:\n\n## fetch some longer price data\nprice_d &lt;- getSymbols('EA', from='2016-01-01', to='2021-12-31', auto.assign = FALSE)\nprice_decomp &lt;- decompose(ts(price_d$EA.Close, frequency=365), type=\"additive\")\nplot(price_decomp)\n\n\n\n\n\n\n\n\n\nwe provide 6 full years of data and most of that is used to calculated decomposition.\nx-axis is year number.\nTREND: trending up to about half-way through year 2, then down until about the same point in year 3, then back up, looking like a peak in mid year 4. Not willing to stretch out beyond that. ;)\nSEASONAL: pattern has been detected where tends to be a dip at beginning of year, rising up to a peak toward end of first quarter, dropping sharply, smaller peak mid-year, peak in q3 or early q4, drop with a smaller bump at end of year.\nRANDOM: as to be expected with stock price in general, lots of randomness involved!\n\nLooks like there may be money to be made riding the seasonal wave! Please: do not buy or sell stocks based on this information. ;)\n\n\nForecasting\nA primary use case for time series objects is forecasting. This is a whole other, involved topic way beyond the scope of this post.\nHere is a quick example to show how easy forecasting can be in R. Note that we need to bring in the forecast package for this. (There is also the amazing [tidyverts eco-system](https://tidyverts.org/) for working with time series that I have recently discovered - again, a whole other topic for another time.)\n\nGet an ARIMA Model\nSome basic terms, over-simplified for our purposes here:\n\nARIMA stands for Auto Regression Integrated Moving Average\nOne of the most widely-used time series forecasting methods, although certainly not the only.\n3 essential parameters for ARIMA are p,d,q: p=periods of lag, d=differencing, q=error of the model.\n\n\nlibrary(forecast)\n## get closing prices for forecasting\nprice_cl &lt;- price[,4]\n## get a model for the time series - using auto.arima for simplicity\nfitA &lt;- auto.arima(price_cl, seasonal=FALSE) ## can add trace=TRUE to see comparison of different models \n## show model\nfitA\n\nSeries: price_cl \nARIMA(0,1,0) \n\nsigma^2 = 5.11:  log likelihood = -1834\nAIC=3671   AICc=3671   BIC=3675\n\n\nThe model we get back is ARIMA(0,1,1) which means p=0, d=1, q=1. We can generate a model by setting these parameters manually, but auto.arima automatically checks a variety of models and selects the best. When comparing models, lowest AIC and BIC are preferred.\nWe can check the accuracy of the model. Most useful item here for interpretation and comparison is MAPE (mean average percent error). In this case,\n\n## check accuracy - based on historical data\naccuracy(fitA)\n\n                 ME RMSE  MAE     MPE MAPE  MASE    ACF1\nTraining set 0.0218 2.26 1.67 0.00169 1.33 0.999 -0.0405\n\nfitAa &lt;- accuracy(fitA)\n100-fitAa[,5]\n\n[1] 98.7\n\n\nSo in this case a MAPE of 1.325 can be seen as accuracy of 98.675%.\nWe can also plot the residuals of the model for visual inspection.\n\n## check residuals\ntsdisplay(residuals(fitA), main='Residuals of Simple ARIMA Forecasting Model for Stock Price')\n\n\n\n\n\n\n\n\nAs usual with residuals, we are looking for mean around 0, roughly evenly distributed. For ARIMA we also get ACF and PACF, where we are looking for bars to be short and at least within blue dotted lines. So looks like we are good to go here.\n\n\nCreate A Forecast\nWe just need a little more code to create and plot forecast. We can set the forecast period for whatever we want, based on the periodicity of the data, in this case days and we are looking out 30 days.\n\ndays=30\nfcastA &lt;- forecast(fitA, h=days)\nplot(fcastA)\n\n\n\n\n\n\n\n\nThat was easy! And we can use this approach to quickly iterate over various models, if we are not convinced that auto.arima is the best. Of course you can use data frames to create forecasts of various sorts but the xts object makes it super-easy to apply common time series methods.\nThis also reveals a shortcoming of times-series forecasting:\n\ndependence of pattern recognition and pattern repetition, which can lead to conservative forecast, especially with noisy data.\nas a result, the forecast is: ‘steady as she goes, with possibility of moving either quite a bit higher or quite a bit lower’.\n\nSo not that useful. To be fair, if stock market prices are not actually predictable, so it is a perfectly reasonable outcome that grounds us in reality.\n\n\n\nConclusion\nTimes series objects are obviously a powerful way to work with time-based data and a go-to when your data is based on time. Particular strengths inculde:\n\nEase of manipulation such as aggregation by date periods, selecting date ranges, period calculations.\nSome great visualization options for exploring the data.\nForecasting which is really the bread and butter of time series objects.\n\nThere are some cases where you may prefer to stick with data frames:\n\nMulti-dimensional data: time series work best when each row represents a distinct time. If you are dealing with multi-dimensional data where dates are broken down by customer, or region, etc., especially in tidy format, you may want to stick with data frame.\nVisualization preferences: if you are more comfortable with using ggplot2 (or other visualization tools geared toward data frames) a data frame may be preferable. Or if the document you are producing has ggplot2 charts, you may want to maintain standard presentation.\nForecasting needs: if you are doing time series forecasting you will want to use a time series object. If you’re not doing forecasting, there is less of a need. Limitation is that time series forecasting is based only on historical trends in the data and doesn’t include things like correlation with other factors.\n\nUltimately, the right tool for the job depends on a variety of situational factors, and having a collection of tools at your disposal helps you avoid the ‘when you have is a hammer…’ pitfall. If your data is based on time, time series should be in consideration.\nSo that’s quite a lot for one blog post - hopefully helps you make the most of your ‘time’!\n\n\nResources\nAdditional resources that may be helpful with time-series and xts in particular:\n\nxts Cheat Sheet.\nsupplementary info. to cheat sheet.\nxts package vignette.\ntime series section in R Cookbook 2nd Edition.\ntsibble package info. - time series for tidyverse."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt02-quarterly-trends/index.html",
    "href": "posts/bc-liquor-market-review-2023-06-pt02-quarterly-trends/index.html",
    "title": "BC Liquor Sales Analysis: Quarter-of-Year Patterns 2015-2023",
    "section": "",
    "text": "(Quarters based on BC LDB fiscal yr Apr-Mar: Q1=Apr, May, Jun. Period is 2015-2023)\n\nBC liquor sales revenue ($) is strongest in Q2 (Jul-Sep of the fiscal yr) and Q3 (Oct-Dec), with generally steep drop-off in Q4 (Jan-Mar) and increase during Q1 (Apr-Jun).\nLitre sales have a strong peak in Q2, driven by summer time beer volumes, whereas consumption shifts toward spirts and wine during holiday season."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt02-quarterly-trends/index.html#highlights",
    "href": "posts/bc-liquor-market-review-2023-06-pt02-quarterly-trends/index.html#highlights",
    "title": "BC Liquor Sales Analysis: Quarter-of-Year Patterns 2015-2023",
    "section": "",
    "text": "(Quarters based on BC LDB fiscal yr Apr-Mar: Q1=Apr, May, Jun. Period is 2015-2023)\n\nBC liquor sales revenue ($) is strongest in Q2 (Jul-Sep of the fiscal yr) and Q3 (Oct-Dec), with generally steep drop-off in Q4 (Jan-Mar) and increase during Q1 (Apr-Jun).\nLitre sales have a strong peak in Q2, driven by summer time beer volumes, whereas consumption shifts toward spirts and wine during holiday season."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt02-quarterly-trends/index.html#intro",
    "href": "posts/bc-liquor-market-review-2023-06-pt02-quarterly-trends/index.html#intro",
    "title": "BC Liquor Sales Analysis: Quarter-of-Year Patterns 2015-2023",
    "section": "Intro",
    "text": "Intro\nThis is a continuation of a previous analysis of annual liquor sales in British Columbia, based on data from British Columbia Liquor Distribution Board ‘Liquor Market Review’. The Liquor Market Review is released on a quarterly basis, covering dollar and litre sales across major categories of beer, wine, spirits, and ‘refreshment beverages’ (ciders, coolers).\nWhile the previous analysis compared year-over-year data, the focus here is on patterns in data based on quarter of year, such as typical peaks and valleys throughout the year, including differences by major beverage type.\nData goes back to 2015 (BC LDB Fiscal Year 2016, since fiscal yr ends in March).\nAs mentioned in previous article: my expertise is in data analysis, not the liquor industry, so the emphasis is on exploring what the data can tell us. Industry-insider context may be lacking. In the interest of promoting data analysis and learning, I am sharing most of the R code used to process the data - hence the expandable ‘Code’ options."
  },
  {
    "objectID": "posts/bc-liquor-market-review-2023-06-pt02-quarterly-trends/index.html#stats-by-quarter-of-the-year",
    "href": "posts/bc-liquor-market-review-2023-06-pt02-quarterly-trends/index.html#stats-by-quarter-of-the-year",
    "title": "BC Liquor Sales Analysis: Quarter-of-Year Patterns 2015-2023",
    "section": "Stats by Quarter of the Year",
    "text": "Stats by Quarter of the Year\n\nOverview\nWe’ll start with an overview across all beverage types and then look at trends for beverage types further down below.\n\n\nCode\n# roll-up categories by qtr\ntrend_ttl_qtr &lt;- lmr_data %&gt;% group_by(year, fyr, fy_qtr, qtr) %&gt;% summarize(\n  netsales=sum(netsales),\n  litres=sum(litres)\n)\n# calculate quarterly info\ntrend_qtr &lt;- lmr_data %&gt;% group_by(fy_qtr, start_qtr_dt, qtr) %&gt;% \n  summarize(netsales=sum(netsales),\n            litres=sum(litres)\n            )\n\ntrend_qtr &lt;- trend_qtr %&gt;% ungroup() %&gt;% mutate(\n            pc_chg_sales=netsales/lag(netsales)-1,\n            pc_chg_litres=litres/lag(litres)-1,\n            pc_chg_sales_qtr=netsales/lag(netsales, n=4),\n            pc_chg_litres_qtr=litres/lag(litres, n=4)\n)\n# averages\ntrend_qtrs &lt;- trend_qtr %&gt;% group_by(qtr) %&gt;% summarize(\n  avgsales=mean(netsales),\n  avglitres=mean(litres),\n  avg_sales_pc_chg=mean(pc_chg_sales, na.rm=TRUE),\n  avg_litres_pc_chg=mean(pc_chg_litres, na.rm=TRUE)\n)\n\n\n\n$ Sales\nOverlay of $ sales data for quarter of each year to see overall trends:\n\nsales are mostly even in Q2 (Jul-Aug-Sep) and Q3 (Oct-Nov-Dec), covering most of the summer and the Christmas holiday season.\nQ4 (Jan-Feb-Mar) by far the slowest period.\n\n\n\nCode\nch_title &lt;- \"$ Sales Comparison by Quarter (fiscal yr end Mar 31)\"\nplot &lt;- trend_ttl_qtr %&gt;% ggplot(aes(x=qtr, y=netsales, color=factor(fyr), group=fyr))+\n  geom_line()+\n  geom_point()+\n  geom_line(data=trend_qtrs, aes(x=qtr, y=avgsales, group=1), color='black', linewidth=1.5)+\n  scale_y_continuous(labels=label_comma(scale=1e-6, prefix=\"$\", suffix=\"M\"), expand=expansion(add=c(0,0.1)), limits=c(0,max(trend_ttl_qtr$netsales)))+\n  labs(title=ch_title, x=\"\", y=\"Net $ Sales\", color='fiscal yr')\nggplotly(plot)\n\n\n\n\nBC LDB fiscal yr end Mar 31, so Q1=Apr-May-Jun, and so on; black line = average.\n\n\nThe drop from Q3 to Q4 is the only consistent pattern across the years looked at. Relative level of sales between Q1-Q2 and Q2-Q3 varies from year to year.\nLooking at patterns in % change between quarters confirms that either ends of the fiscal yr have the biggest changes:\n\nQ4 has a consistently large drop from previous quarter and Q1 has consistently big increase from previous quarter.\nQ2 tends to show positive growth over Q1, but not always. Q3 straddles 0% change from previous.\n\n\n\nCode\nch_title &lt;- \"Distribution of % Change in $ Sales Between Quarters\"\nplot &lt;- trend_qtr %&gt;% ggplot(aes(y=pc_chg_sales, x=qtr))+\n  geom_boxplot(fill=bar_col)+\n  scale_y_continuous(labels=percent_format())+\n  geom_hline(yintercept = 0, linetype='dotted')+\n  labs(title=ch_title, x=\"\",y=\"% chg from prev quarter\")\nggplotly(plot)\n\n\n\n\n\n\nThe middle black line represents median for each quarter, with the colored area representing the range from 25% of the time to 75% of the time (1st & 3rd quartiles).\n\n\nLitres\nOverlaying litres sold by quarter for each year shows volume changes throughout the year:\n\nQ2 (Jul-Aug-Sep) has highest sales, edging out Q1 (Apr-May-Jun)\nQ3 (Oct-Nov-Dec) sees drop, even with holidays, and then lower again for Q4 (Jan-Feb-Mar)\n\n\n\nCode\nch_title &lt;- \"Litres Comparison by Quarter (fiscal yr end Mar 31)\"\nplot &lt;- trend_ttl_qtr %&gt;% ggplot(aes(x=qtr, y=litres, color=factor(fyr), group=fyr))+\n  geom_line()+\n  geom_point()+\n  geom_line(data=trend_qtrs, aes(x=qtr, y=avglitres, group=1), color='black', size=1.5)+\n  scale_y_continuous(labels=label_comma(scale=1e-6, suffix=\"M\"), expand=expansion(add=c(0,0.1)), limits=c(0,max(trend_ttl_qtr$litres)))+\n  labs(title=ch_title, x=\"\", y=\"litres\", color='fiscal yr')\nggplotly(plot)\n\n\n\n\nBC LDB fiscal yr ends Mar 31\n\n\nThe different pattern compared to $ sales likely represents the shift in drinking habits with the seasons:\n\npeak volume (litres) is in Q2 - beer season, and beer, along with refreshment beverages, are consumer in higher volumes than the other types.\nQ3, which is usually even with Q2 in $ sales, has a consistent drop in litre volume compared to Q2, due to shift away from beer/refreshments beverages toward wine and spirits.\n\nPatterns by beverage type are looked at more below.\nPercentage changes from one quarter to the next reflect the different quarter-by-quarter patterns for litres.\n\n\nCode\nch_title &lt;- \"Distribution of % Change in Litres Between Quarters\"\nplot &lt;- trend_qtr %&gt;% ggplot(aes(y=pc_chg_litres, x=qtr))+\n  geom_boxplot(fill=bar_col)+\n  scale_y_continuous(labels=percent_format())+\n  geom_hline(yintercept = 0, linetype='dotted')+\n  labs(title=ch_title, x=\"\",y=\"% chg from prev quarter\")\nggplotly(plot)\n\n\n\n\n\n\n\nQ1 (Apr-May-Jun) typically has around 35% increase over previous Q4\nQ2 (Jul-Aug-Sep) tends to be even or slight increase over Q1.\nQ3 (Oct-Nov-Dec) big drop around 15% range compared to Q2.\nQ4 (Jan-Feb-Mar) usually another ~15% from Q3 to Q4.\n\n\n\n\nBy Major Beverage Type\nLet’s look at beverage types to see what is going on below the surface of overall trends.\n\n\nCode\n# quarters at cat_type level\ntrend_qtr_cat &lt;- lmr_data %&gt;% group_by(year, fyr, fy_qtr, start_qtr_dt, qtr, type) %&gt;%\n  summarize(netsales=sum(netsales),\n            litres=sum(litres)\n            )\ntrend_qtr_cat$type &lt;- as.factor(trend_qtr_cat$type)\n\ntrend_qtrs_cat &lt;- trend_qtr_cat %&gt;% group_by(qtr, type) %&gt;% \n  summarize(\n    avg_netsales=mean(netsales),\n    avg_litres=mean(litres),\n    max_litres=max(litres)\n  )\ntrend_qtrs_cat &lt;- trend_qtrs_cat %&gt;% group_by(qtr) %&gt;% mutate(\n  avg_netsales_pc=avg_netsales/sum(avg_netsales),\n  avg_litre_pc=avg_litres/sum(avg_litres)\n)\ntrend_qtrs_cat$type &lt;- as.factor(trend_qtrs_cat$type)\n\n\n\n$ Sales\nLooking at quarter sales by beverage type reveals patterns we suspected earlier:\n\nBeer and refreshment beverages tend to peak in Q2 and decline in Q3 and Q4.\nWine and, especially, spirits peak in Q3 and offset the decline in revenue from beer and refreshment beverages.\n\n\n\nCode\nch_title &lt;- \"Comparison of Qtrly $ Sales by Beverage Type\"\ntrend_qtr_cat %&gt;% ggplot(aes(x=qtr, y=netsales, color=factor(fyr), group=fyr))+\n  geom_line()+\n  geom_point()+\n  geom_line(data=trend_qtrs_cat, aes(x=qtr, y=avg_netsales), color='black', group=1, size=1.2)+\n  facet_grid(type~., scales='free_y')+\n  scale_y_continuous(labels=label_comma(scale=1e-6, prefix=\"$\", suffix=\"M\"), \n                     limits=c(0,NA))+\n  labs(title=ch_title, x=\"\", color=\"fiscal yr\", y=\"$ Sales (upper limit varies)\")+\n  theme(panel.border = element_rect(fill=NA))\n\n\n\n\n\nblack line = average\n\n\n\n\nStacked chart highlights the shifts between Q2 and Q3:\n\ndecline in $ sales of Beer and Refreshment Beverages in Q3 almost completely offset by increase in wine and spirits.\n\n\n\nCode\ntrend_qtrs_cat$type &lt;- fct_reorder(trend_qtrs_cat$type, trend_qtrs_cat$avg_netsales)\n\nch_title &lt;- \"$ Sales by Type\"\nplot &lt;- trend_qtrs_cat %&gt;% ggplot(aes(x=qtr, y=avg_netsales, fill=type))+\n  #geom_col(position='fill')+\n  geom_col()+\n  scale_y_continuous(labels=label_comma(scale=1e-6, prefix=\"$\", suffix=\"M\"), \n                     expand=expansion(add=c(0,0.1)))+\n  scale_fill_manual(values=type_color)+\n  labs(title=ch_title, x=\"\",y=\"Avg Net Sales in Qtr (2015-2023)\")+\n  theme(axis.ticks.x = element_blank(),\n        axis.title.y=element_text(size=9))\np1_plotly &lt;- ggplotly(plot) %&gt;% layout(margin=list(l=110, r=0), showlegend=TRUE,\n                                       legend=list(x=-1, y=0.6, xanchor='left', \n                                                   yanchor='middle'))\n\nch_title &lt;- \"% $ Sales by Type\"\nplot_f &lt;- trend_qtrs_cat %&gt;% ggplot(aes(x=qtr, y=avg_netsales, fill=type))+\n  geom_col(position='fill')+\n  scale_y_continuous(labels=percent_format(), expand=expansion(add=c(0,0)))+\n  scale_fill_manual(values=type_color)+\n  labs(title=ch_title, x=\"\",y=\"Avg % Net Sales in Qtr (2015-2023)\")+\n  theme(axis.ticks.x = element_blank(),\n        axis.title.y=element_text(size=9))\np2_plotly &lt;- ggplotly(plot_f) %&gt;% layout(margin=list(l=60, r=0), showlegend=FALSE)\n# grid.arrange doesn't work with plotly; plotly has subplot, but couldn't get right layout\nmanipulateWidget::combineWidgets(p1_plotly, p2_plotly, nrow=1, colsize = c(3,2))\n\n\n\n\n\n\n\n\nLitres\nSimilar quarter-over-quarter patterns can be seen when looking at litres sold, although the overall differences from one quarter to the next - especially Q2 to Q3 - are larger when measured in litres, since Beer and Refreshment Beverages are consumed in larger quantities than Wine and Spirits.\n\n\nCode\nch_title &lt;- \"Comparison of Qtrly volume by Category\"\ntrend_qtr_cat %&gt;% ggplot(aes(x=qtr, y=litres, color=factor(fyr), group=fyr))+\n  geom_line()+\n  geom_point()+\n  facet_grid(type~., scales='free_y')+\n  scale_y_continuous(labels=label_comma(scale=1e-6, suffix=\"M\"), \n                     limits=c(0,NA))+\n  labs(title=ch_title, x=\"\", color=\"fiscal yr\", y=\"litres (upper limit varies)\")+theme(panel.border = element_rect(fill=NA))\n\n\n\n\n\n\n\n\n\nCode\n  #+ theme(panel.border = element_line())\n\n\n\nBeer and, even more-so, refreshment beverages drop from Q1-Q2 (Apr-Sep) to Q3-Q4 (Oct-Mar).\nWine, Spirits have small peak in Q3 (Oct-Dec) - holiday season.\n\nThis pattern shows up clearly when looking at the average % breakdown in litres by beverage type (chart on right):\n\n\nCode\ntrend_qtrs_cat$type &lt;- fct_reorder(trend_qtrs_cat$type, trend_qtrs_cat$avg_litres)\n\nch_title &lt;- \"Avg Litres by Type\"\nplot &lt;- trend_qtrs_cat %&gt;% ggplot(aes(x=qtr, y=avg_litres, fill=type))+\n  geom_col()+\n  scale_y_continuous(labels=label_comma(scale=1e-6, suffix=\"M\"), \n                     expand=expansion(mult = c(0,0)))+\n  scale_fill_manual(values=type_color)+\n  labs(title=ch_title, x=\"\",y=\"Average Litres by Quarter (2015-2023)\")+\n  theme(axis.ticks.x = element_blank(),\n        axis.title.y = element_text(size=9),\n        legend.position = 'none')\nplotly_1 &lt;- ggplotly(plot)\n\nch_title &lt;- \"Avg Litre % by Type\"\nplot &lt;- trend_qtrs_cat %&gt;% ggplot(aes(x=qtr, y=round(avg_litre_pc,2), fill=type))+\n  geom_col(position='fill')+\n  scale_y_continuous(labels=percent_format(), expand=expansion(mult = c(0,0)))+\n  scale_fill_manual(values=type_color)+\n  labs(title=ch_title, x=\"\",y=\"Average % Share by Quarter (2015-2023)\")+\n  theme(axis.ticks.x = element_blank(),\n        axis.title.y = element_text(size=9),\n        legend.position = 'right')\nplotly_2 &lt;- ggplotly(plot)\ncombineWidgets(plotly_1, plotly_2, nrow=1, colsize = c(3,5))\n\n\n\n\n\n\nChart on right highlights the bulge in % litre share for Wine in Q3 (Sep-Dec), an increase from 15% in previous quarter to 20% in Q3. Spirts likewise have strong increase from 6% in Q2 to 8% in Q3. These shifts in share come at the expense largely of Refreshment Beverages - dropping from 16% in Q2 to 11% in Q3 - and Beer, to a lesser extent, down only 2%. It seems refreshment beverage drinkers are somewhat fickle and maybe make choices based on the season.\n\n\n\nPart 2 Wrap-up and Next Up\nThis concludes our look at quarter patterns, including quarters from mid-2015 to mid-2023.\n\nNext-up:\n\nCategory trends and patterns: closer look at each of the major beverage types, exploring categories and sub-categories within them, as reported in the Liquor Market Review.\nCategory 1: Beer: start with beer, because…beer. ;)\n\n\n\n\nFootnotes\nNotes on ‘net $ sales’:\n\nthe report says “Net dollar value is based on the price paid by the customer and excludes any applicable taxes.”\ncalculating average net dollar value per litre for beverage categories gives unrealistically low numbers compared to retail prices in BC liquor stores. (Beer at average $4/litre? Not even the cheapest beer on the BC Liquor Stores website.)\nthere is likely additional factors related to BC LDB pricing structure, wholesaling, etc.\nbest to consider average net dollar value per litre referred to below as relative indicator."
  },
  {
    "objectID": "posts/ggplot-theme-sampler/index.html#function-to-take-chart-info-and-process-themes",
    "href": "posts/ggplot-theme-sampler/index.html#function-to-take-chart-info-and-process-themes",
    "title": "GGplot Theme Sampler: Choosing a Theme",
    "section": "Function to take chart info and process themes",
    "text": "Function to take chart info and process themes\nA reusable function for ease of sampling different themes with different chart types.\n\n\nCode\n# function to take chart info as input, apply themes, return variations with themes\nchartThemeSampler &lt;- function(chart_example) {\n  c_dflt &lt;- chart_example + labs(title = \"Default (aka Gray/Grey)\")\n  c_m &lt;- chart_example + labs(title = \"Minimal\") + theme_minimal()\n  c_l &lt;- chart_example + labs(title = \"Light\") + theme_light()\n  c_drk &lt;- chart_example + labs(title = \"Dark\") + theme_dark()\n  c_c &lt;- chart_example + labs(title = \"Classic\") + theme_classic()\n  c_bw &lt;- chart_example + labs(title = \"BW\") + theme_bw()\n  # ggthemes\n  c_econw &lt;- chart_example + labs(title = \"economist_white (ggt)\")+\n            theme_economist_white()\n  c_few &lt;- chart_example + labs(title = \"Few (ggt)\") + theme_few()\n  c_five &lt;- chart_example + labs(title = \"FiveThirtyEight (ggt)\") + theme_fivethirtyeight()\n  c_gdoc &lt;- chart_example + labs(title = \"GDocs (ggt)\") + theme_gdocs()\n  c_hc &lt;- chart_example + labs(title = \"HC (ggt)\") + theme_hc()\n  c_sol2 &lt;- chart_example + labs(title = \"Solarized  2(ggt)\") + theme_solarized_2()\n  c_tufte &lt;- chart_example + labs(title = \"theme_tufte (ggthemes)\") + theme_tufte()\n  c_wsj &lt;- chart_example + labs(title = \"WSJ (ggt)\") + theme_wsj()\n  ## ggpubr\n  c_clev &lt;- chart_example + labs(title = \"Cleveland (ggp)\") + theme_cleveland()\n    # theme_transparent requires extra code to display title\n  c_transp &lt;- chart_example + labs(title = \"Transparent (ggp)\") + theme_transparent() + \n    theme(plot.title = element_text())\n  \n  # combine\n  cts &lt;- list(c_dflt, c_m, c_l, c_drk, c_c, c_bw, \n              c_econw, c_few, c_five, c_gdoc, c_hc, c_sol2, c_tufte, c_wsj,\n              c_clev, c_transp)\n  for(i in 1:length(cts)) {\n    print(cts[[i]])\n  }\n}"
  },
  {
    "objectID": "posts/ggplot-theme-sampler/index.html#histograms",
    "href": "posts/ggplot-theme-sampler/index.html#histograms",
    "title": "GGplot Theme Sampler: Choosing a Theme",
    "section": "Histograms",
    "text": "Histograms\n# set up chart for applying themes\nchart_example &lt;- chart_data_select %&gt;% ggplot(aes(x=Growth))+geom_histogram()\n# use function to apply themes to the charts and display\nchartThemeSampler(chart_example)"
  },
  {
    "objectID": "posts/ggplot-theme-sampler/index.html#line-charts",
    "href": "posts/ggplot-theme-sampler/index.html#line-charts",
    "title": "GGplot Theme Sampler: Choosing a Theme",
    "section": "Line Charts",
    "text": "Line Charts\nchart_example &lt;- chart_data_single %&gt;% ggplot(aes(x=Year, y=Growth))+geom_line()\nchartThemeSampler(chart_example)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLine charts w/Legend\nchart_example &lt;- chart_data_select %&gt;% ggplot(aes(x=Year, y=Growth, col=Code))+geom_line()\nchartThemeSampler(chart_example)"
  },
  {
    "objectID": "posts/ggplot-theme-sampler/index.html#column-bar-charts",
    "href": "posts/ggplot-theme-sampler/index.html#column-bar-charts",
    "title": "GGplot Theme Sampler: Choosing a Theme",
    "section": "Column (Bar) Charts",
    "text": "Column (Bar) Charts\nchart_example &lt;- chart_data_single %&gt;% ggplot(aes(x=Year, y=Growth))+geom_col()\nchartThemeSampler(chart_example)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Charts - Facets\nchart_example &lt;- chart_data_select %&gt;% \n  filter(Year &gt;= 2000 & Code %in% c(\"BRA\",\"CAN\",\"IND\")) %&gt;%\n  ggplot(aes(x=Year, y=Growth))+geom_col()+\n  facet_grid(Code~.)\nchartThemeSampler(chart_example)"
  },
  {
    "objectID": "posts/ggplot-theme-sampler/index.html#boxplots",
    "href": "posts/ggplot-theme-sampler/index.html#boxplots",
    "title": "GGplot Theme Sampler: Choosing a Theme",
    "section": "Boxplots",
    "text": "Boxplots\nchart_example &lt;- chart_data_select %&gt;% \n  ggplot(aes(x=Code, y=Growth))+geom_boxplot()\nchartThemeSampler(chart_example)"
  },
  {
    "objectID": "posts/ggplot-theme-sampler/index.html#scatterplots",
    "href": "posts/ggplot-theme-sampler/index.html#scatterplots",
    "title": "GGplot Theme Sampler: Choosing a Theme",
    "section": "Scatterplots",
    "text": "Scatterplots\nchart_example &lt;- chart_data_single %&gt;% ggplot(aes(x=GDP_per_cap, y=Growth))+geom_point()\nchartThemeSampler(chart_example)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatterplots - Facets\nchart_example &lt;- chart_data_select %&gt;% filter(Code %in% c(\"BRA\",\"CAN\",\"IND\")) %&gt;%\n  ggplot(aes(x=Exports, y=Imports))+geom_point()+\n  facet_grid(Code~.)\nchartThemeSampler(chart_example)"
  },
  {
    "objectID": "posts/ggplot-theme-sampler/index.html#mods-experiments",
    "href": "posts/ggplot-theme-sampler/index.html#mods-experiments",
    "title": "GGplot Theme Sampler: Choosing a Theme",
    "section": "Mods / Experiments",
    "text": "Mods / Experiments\nClearly, there are lots of options readily available to suit most situations. All of these themes have their strengths and limitations.\nStill, it is entirely possible that none of the ‘out-of-the-box’ themes may fit exactly with what you want. Here are some examples of minor modifications to built-in themes to see how they play out across different chart types.\n\nChart Setup & Function for mods\n\nChart setup\nA collection of charts to apply themes to, as opposed to applying various themes to a specific chart, as above.\n\n\nCode\nchart_line &lt;- chart_data_single %&gt;% ggplot(aes(x=Year, y=GDP)) + geom_line()\nchart_col &lt;- chart_data_single %&gt;% ggplot(aes(x=Year, y=GDP)) + geom_col()\nchart_scat &lt;- chart_data_select %&gt;% ggplot(aes(x=Exports, y=Growth)) + geom_point()\nchart_box &lt;- chart_data_select %&gt;% ggplot(aes(x=Code, y=Growth)) + geom_boxplot()\n# facets\nchart_col_facet &lt;- chart_data_select %&gt;% \n  filter(Year &gt;= 2000 & Code %in% c(\"JPN\",\"NGA\",\"USA\")) %&gt;%\n  ggplot(aes(x=Year, y=Growth))+\n  geom_col()+\n  facet_grid(Code~.)\nchart_scat_facet &lt;- chart_data_select %&gt;% filter(Code %in% c(\"BRA\",\"CAN\",\"IND\")) %&gt;%\n  ggplot(aes(x=Imports, y=Growth)) + geom_point() +\n  facet_grid(Code~.)\n\n\n\n\nFunction for processing themes\n\n\nCode\n# takes list of chart settings plus additional theme definition as inputs and combines\nchartMod &lt;- function(charts, chart_mods, theme_def) {\n  for(c in 1:length(charts)) {\n    chart_modo &lt;- charts[[c]] + chart_mods + theme_def\n    print(chart_modo)\n  }\n}\n\n\n\n\n\nLight without grid lines\n\ntheme_light, with border\nno gridlines\ngeom_hline for line at 0 on y-axis\n\n# set list of charts\ncharts &lt;- list(chart_line, chart_col, chart_scat, chart_box, chart_col_facet, chart_scat_facet)\n# add chart modifications\nchart_mods &lt;- geom_hline(yintercept = 0, color='grey90')\n# set theme to apply\ntheme_def &lt;- theme_light() + theme(panel.grid = element_blank())\n# run function to show charts\nchartMod(charts, chart_mods, theme_def)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLight wth major gridlines only\nMajor gridlines for reference with color lightened and size reduced. 0 line for extra context, distinguished from gridlines.\ncharts &lt;- list(chart_line, chart_col, chart_scat, \n               chart_box, chart_col_facet, chart_scat_facet)\nchart_mods &lt;- geom_hline(yintercept = 0, color = 'grey60') \ntheme_def &lt;- theme_light() + theme(panel.grid.major = \n                                     element_line(color = 'grey80', linewidth = 0.1),\n                                  panel.grid.minor = element_blank())\nchartMod(charts, chart_mods, theme_def)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFew with reference line\n\nReference line at 0 for context.\nEssentially same as ‘Light’ without gridlines, above, but has some cleaner labelling, notable on facets.\n\ncharts &lt;- list(chart_line, chart_col, chart_scat, \n               chart_box, chart_col_facet, chart_scat_facet)\nchart_mods &lt;- geom_hline(yintercept = 0, color = 'grey70') \ntheme_def &lt;- theme_few()\nchartMod(charts, chart_mods, theme_def)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransparent with border\n\nExtremely minimal, could be useful in showing general pattern, without details.\nBorder may help to provide some sense of ‘boundary area’\n\ncharts &lt;- list(chart_line, chart_col, chart_scat, \n               chart_box, chart_col_facet, chart_scat_facet)\nchart_mods &lt;- geom_hline(yintercept = 0, color = \"grey90\") \ntheme_def &lt;- theme_transparent()+theme(panel.border = element_rect(size=1, color='grey90', fill=NA))\nchartMod(charts, chart_mods, theme_def)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolarized_2 with major grid only\n‘0’ line highlighted.\ncharts &lt;- list(chart_line, chart_col, chart_scat, \n               chart_box, chart_col_facet, chart_scat_facet)\nchart_mods &lt;- geom_hline(yintercept = 0, color=\"white\") \ntheme_def &lt;- theme_solarized_2() + theme(panel.grid.minor = element_blank())\nchartMod(charts, chart_mods, theme_def)"
  },
  {
    "objectID": "posts/ggplot-theme-sampler/index.html#closing-suggestions",
    "href": "posts/ggplot-theme-sampler/index.html#closing-suggestions",
    "title": "GGplot Theme Sampler: Choosing a Theme",
    "section": "Closing suggestions",
    "text": "Closing suggestions\n\nconsider your objectives, audience and priorities:\n\nif time is of the essence, a built-in chart or even default may be good enough\nif design is important to your audience, you may want to go the extra distance to craft something more specialized\nif looking to create a personal or professional brand, may want to create your own template that you can use repeatedly over time and different situations\n\nkeep things as simple as possible, but no simpler: provide enough guidance to direct and assist the eye\nconsider reference lines in addition to / instead of gridlines to provide easier anchoring\n\nAbove all else, for the sake of your data consumers…keep on visualizing!"
  },
  {
    "objectID": "posts/quarto-shinylive-experiment/index.html",
    "href": "posts/quarto-shinylive-experiment/index.html",
    "title": "Quarto Shinylive Experiment: Promising for Limited Use",
    "section": "",
    "text": "Summary\n\n\n\nSimple example demonstrates the benefits of embedding Shiny apps in Quarto documents. This technology has some great potential for expanding the use of Shiny apps, although use cases currently limited by lack of ability to pass data in/out of the shiny app."
  },
  {
    "objectID": "posts/quarto-shinylive-experiment/index.html#quarto-shinylive",
    "href": "posts/quarto-shinylive-experiment/index.html#quarto-shinylive",
    "title": "Quarto Shinylive Experiment: Promising for Limited Use",
    "section": "Quarto + Shinylive!",
    "text": "Quarto + Shinylive!\nTaking Quarto docs to the next level by embedding live, fully interactive Shiny apps! An experiment inspired by Joe Cheng’s presentation ‘Running R-Shiny without a Server’ at posit::conf(2023). (20 min video)\nTurn out that:\n\nIt is fairly straightforward to implement\n\nAdditional reference:\n\nTo learn more about Shinylive see github.com/posit-dev/r-shinylive.\nTry online version of Shinylive for R and see examples at shinylive.io/r/examples/\nTo learn more about Quarto see quarto.org.\n\nNote: this project set up using ‘.renv’ which is good for reproducibility, but may cause complications for package management on different machines over time."
  },
  {
    "objectID": "posts/quarto-shinylive-experiment/index.html#swiss-dataset",
    "href": "posts/quarto-shinylive-experiment/index.html#swiss-dataset",
    "title": "Quarto Shinylive Experiment: Promising for Limited Use",
    "section": "Swiss dataset",
    "text": "Swiss dataset\nR built-in swiss dataset with fertility and socio-economic indicators by province, from 1888.\n\n\nCode\nswiss &lt;- swiss\nswiss$prov &lt;- rownames(swiss)\nswiss_top &lt;- swiss %&gt;% arrange(-Fertility) %&gt;% slice_head(n=10)\n# save with relative location for importing to shiny app below -&gt; doesn't help\nwrite_csv(swiss_top, 'data/swiss_top.csv')"
  },
  {
    "objectID": "posts/quarto-shinylive-experiment/index.html#visualize",
    "href": "posts/quarto-shinylive-experiment/index.html#visualize",
    "title": "Quarto Shinylive Experiment: Promising for Limited Use",
    "section": "Visualize",
    "text": "Visualize\nTypical static plot produced with ggplot2: useful, but limited:\n\n\nCode\nswiss_top %&gt;% ggplot(aes(x=reorder(prov,Fertility), y=Fertility))+geom_col()+\n  geom_hline(yintercept=mean(swiss_top$Fertility), linetype='dashed', color='green')+\n  coord_flip()+\n  scale_y_continuous(expand=expansion(mult=c(0,0.02)))+\n  labs(title='Top 10 Swiss Provinces by Fertility', x=\"\", \n       subtitle = '(births per 1,000 women; dotted line = average)')+\n  theme_light()+\n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_text(size=11))\n\n\n\n\n\n\n\n\nFigure 1: from Swiss Fertility & Socioeconomic Indicators (1888); built-in R dataset."
  },
  {
    "objectID": "posts/quarto-shinylive-experiment/index.html#now-with-shinylive",
    "href": "posts/quarto-shinylive-experiment/index.html#now-with-shinylive",
    "title": "Quarto Shinylive Experiment: Promising for Limited Use",
    "section": "Now with Shinylive!",
    "text": "Now with Shinylive!\nInteractive, filterable version of the chart that leverages R shiny plus webR technology to display in browser…without the need for shiny server!\nMay take a while to load…\n(apologies if the ‘sidebar’ filters are stacked above the chart - appears to be result of narrow width of the theme I’m using and not improved by setting widths of side/main panels; worked as expected in other experiments)\n#| standalone: true\n#| viewerHeight: 700\n\n# load packages\nlibrary(shiny)\nlibrary(datasets)\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(here)\n\n# get data - import saved file with relative location\n#swiss_top &lt;- read_csv('data/swiss_top.csv') # failed attempt at reading data\nswiss &lt;- datasets::swiss\nswiss$prov &lt;- rownames(swiss)\nswiss &lt;- swiss %&gt;% arrange(-Fertility)\n\n# Define shiny ui\nui &lt;- fluidPage(\n  # shiny UI components here\n  # Application title\n  titlePanel(\"Swiss Fertility Data by Province\"),\n  \n  # Sidebar layout with input and output definitions\n  sidebarLayout(\n    # Sidebar panel for inputs\n    sidebarPanel(\n      width=3, ## setting worked nicely in other quarto docs - no respond here\n      # Input: number of provinces to show (since 47 total)\n      numericInput(inputId='num_prov', \n                   label='No. of Provs. to show',\n                   value=10, min=1, max=50, step=1),\n      # Input: checkbox for the regions to plot - dynamic based on num_prov\n      uiOutput('dynamicCheckbox'),\n      p('note: 47 provinces in total')\n      ), # end sidebarPanel\n    \n    # Main panel for displaying outputs\n    mainPanel(\n      width=9,\n      h3('Swiss Fertility'),\n      # Output: Column chart rendered with ggplot2\n      plotOutput(outputId = \"fert\", height=\"540px\")\n    ) # end mainPanel\n  )\n)\n\n# Define shiny server logic here  \nserver &lt;- function(input, output, session) {\n  # shiny server code\n  output$dynamicCheckbox &lt;- renderUI({\n     num_provinces &lt;- input$num_prov\n     checkboxGroupInput(inputId=\"prov\", \"Select Provinces (desc order of fertility)\", \n                        choices = head(swiss$prov, num_provinces),\n                        select=swiss$prov)\n   })\n  # Reactive expression to generate the plot based on the inputs\n  output$fert &lt;- renderPlot({\n    # filter provinces using checklist and num_prov selector\n    swiss_top &lt;- swiss %&gt;% filter(prov %in% input$prov)\n    # Generate ggplot2 column chart\n    swiss_top |&gt; ggplot(aes(x=reorder(prov, Fertility), y=Fertility))+\n      geom_col()+\n      geom_hline(yintercept=mean(swiss_top$Fertility), \n                 linetype='dashed', color='green')+\n      coord_flip()+\n      scale_y_continuous(expand=expansion(mult=c(0,0.05)))+\n      labs(title='Swiss Provinces by Fertility (1888)', \n           subtitle = '(births per 1,000 women; dotted line = average)',\n           x=\"\")+\n      theme_light()+\n      theme(axis.ticks.y = element_blank(),\n            axis.text.y = element_text(size=12))\n   })\n}\n\n# create and launch shiny app\nshinyApp(ui = ui, server = server)\nThis is implemented in just a few steps:\n\nAdd the Quarto shinylive extension to your project.\n\nterminal: quarto add quarto-ext/shinylive\n\nGet the shinylive package.\nAdd filter to document yaml header\n\n\n\n\n\n{shinylive-r} code block to hold the shiny code:\n\n#| standalone: true\n#| viewerHeight: 600 - ensures app window is large enough; adjust to pref\nlibrary(shiny)\nstandard shiny code for a single-file app within the code block (ui, server)\n\n\nThat’s it! Render the page and enjoy the view - and the interaction possibilities. (Will need to use ‘show in new window’ option, as it won’t display correctly in the Viewer panel.)\nCode display doesn’t work with the {shinylive-r} code block, so showing the skeleton code below, with key components. (actual code is long for this page - you can see similar examples in my quarto-shiny-live-examples Github repo.)\n\n\nCode\n{shinylive-r}\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Swiss Fertility Data by Province\"),\n  sidebarLayout(\n    sidebarPanel(\n      inputs\n    ),\n    mainPanel(\n      plotOutput(outputId = \"fert\")\n    )\n  )\n)\n\n# Define shiny server logic here  \nserver &lt;- function(input, output, session) {\n  output$fert &lt;- renderPlot({\n   })\n}\n\n# create and launch shiny app\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/quarto-shinylive-experiment/index.html#but-not-so-fast",
    "href": "posts/quarto-shinylive-experiment/index.html#but-not-so-fast",
    "title": "Quarto Shinylive Experiment: Promising for Limited Use",
    "section": "But: Not So Fast…",
    "text": "But: Not So Fast…\nThere are some significant limitations.\n\nWorking with data\nBy far the most significant, as I’m as I’m concerned:\n\nnot able to load external data from outside the shiny app (as far as I can tell)\n\nno import csv (even local to the quarto project)\nno database connection\nno read googlesheet\n\nI can only use data generated within the app OR built-in R datasets\n\n(hence the use of the swiss dataset here)\n\n\nI haven’t been able to figure a way to import data to the app, despite attempting many approaches. So this is a deal-breaker for a lot applications - pretty much all of the use cases I would have.\n\n\nOther limitations:\n\nsingle file app, so limited complexity can be developed\nhard to debug - no error messages or other clues when app fails\n\ncan use shinylive.io for testing\n\nnot all R packages available - but most, so shouldn’t be major blocker\nslow loading time - a nuisance, but generally not unbearable\nrestricted size: limited by format of quarto document (since embedded within Quarto doc layout)\n\nNote that these limitations apply specifically to using Shinylive for embedding into Quarto documents. This is only one use case. Others include:\n\nshinylive.io: for prototyping, potentially sharing apps.\nshiny app conversion: from regular shiny app that needs a server to serverless app that can be shared more easily."
  },
  {
    "objectID": "posts/quarto-shinylive-experiment/index.html#conclusion",
    "href": "posts/quarto-shinylive-experiment/index.html#conclusion",
    "title": "Quarto Shinylive Experiment: Promising for Limited Use",
    "section": "Conclusion",
    "text": "Conclusion\nShinylive is a powerful new technology that holds lots of potential for embedding Shiny apps into Quarto documents for interactive data exploration. Currently, though, it has limited application. This will likely be further developed and become even more valuable over time. Maybe not ready for primetime now, but will have to keep an eye on this!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Who am I?\nI’m John Yuill. I have close to 20 years of working with data to improve decision-making, in agency, consulting, and in-house roles. Most recently managing a marketing analytics team at EA in Vancouver, BC.\n\n\nWhat is Catbird Analytics?\nBeing a catbird is all about operating from the proverbial ‘catbird seat’: a vantage point that is far enough above the fray to see the big picture, while being close enough to the action to understand the details, maybe even swoop down to ground level once in a while, then return back to a perch to survey the situation.\nAnd when you combine that with the power of data & analysis, you get Catbird Analytics."
  },
  {
    "objectID": "posts/ggplot-theme-sampler/index.html",
    "href": "posts/ggplot-theme-sampler/index.html",
    "title": "GGplot Theme Sampler: Choosing a Theme",
    "section": "",
    "text": "A small selection of themes available for ggplot2\n\n\n\n\nSooner or later, the times comes to move beyond the default ggplot2 grey theme and explore other options. But which theme to use? And When?\nHere is a collection of ggplot charts with different themes applied, intended to provide some helpful comparisons and make it easier to choose the best option for the situation.\nThe focus is on themes that come with the ggplot package, but also included are some examples from ‘ggthemes’ and ‘ggpubr’ packages. Consideration could be extended to other packages like ‘hrbrthemes’, ‘ggthemr’, ‘themetron’ or others. Although the possibilities are endless, realistically, many of the themes are variations on existing themes/concepts and it may be just as easy to work with a built-in theme and tweak it to your own desired flavour. Some examples and thoughts on this at the end.\n\n\n\n\nggplot2 built-in: if none of the below specified.\nggthemes library: identified by ‘ggt’ in chart titles below.\nggpubr library: identified by ‘ggp’ in chart titles below.\n\nthe ‘transparent’ theme suppresses title by default, so extra modification was needed. if used ‘out of the box’, will not have a title, even if title is specified.\n\n\n\n\n\nDefault ggplot colors used for expediency - exploring color palettes is another adventure altogether."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Examples of the types of projects I work on.\n\n\n\nProject One\nThis is the thing you are looking for, I’m sure. The title is good style.\n\n\n\n\nAnother Thing\nThis is another thing you are probably looking for. It is quite interesting if you look at it. More to come on this. H4 is brash but enables linking more easily than non-header.\n\n\n\n\n\nThisTitlehere Here\nThis is what it is called and what it does."
  },
  {
    "objectID": "portfolio-html.html",
    "href": "portfolio-html.html",
    "title": "Portfolio",
    "section": "",
    "text": "Here are some projects representing the types of things I work on:\n\n\n\n\n\n\nThis You Need to See\n\n\nNot only that it has everything.\n\n\n\n\n\n\n\nSerious This\n\n\nThis one you really need to see to believe. Please.\n\n\n\n\n\n\n\nTime Capsule\n\n\nLike under the bridge\n\n\n\n\n\n\n\nTime Capsule\n\n\nLike under the bridge\n\n\n\n\n\n\n\nTime Capsule Again\n\n\nLike under the bridge but not as much as before."
  },
  {
    "objectID": "portfolio-html2.html",
    "href": "portfolio-html2.html",
    "title": "Portfolio Test",
    "section": "",
    "text": "Testing for portfolio page. Uses pure HTML and CSS with flexbox styles.\n\n\n\nin quarto, spacing can matter in HTML code\n\n\nswitching back and forth from Source to Visual can mees things up!\n\n\nusing style names like ‘card’ that may be used by boostrap or other framework can produce unexpected effects (but these can be overridden in CSS)\n\n\n\n\ncategory 1\n\n\n \n\nProject 1\n\n\nthis is the thing\n\n\n \n\nProject 2\n\n\nthis is the only thing\n\n\n \n\nProject 3\n\n\nthis is the other thing\n\n\n \n\nProject 4\n\n\nthis is thing using card\n\n\n \n\nProject 5\n\n\nthis is thing using card2\n\n\n \n\nProject 6\n\n\nthis is the other thing with car\n\n\n\n\nthis category 2\n\n\n \n\nProject 1\n\n\nthis is the thing\n\n\n \n\nProject 2\n\n\nthis is the only thing\n\n\n \n\nProject 3\n\n\nthis is the other thing\n\n\n\n\nSmaller cards more cols\n\n\nSmaller size automatically fills up space avail.:\n\n\n \n\nProject 1\n\n\nShort description here.\n\n\n \n\nProject 2\n\n\nAnother brief detail.\n\n\n \n\nProject 3\n\n\nSomething cool about this one.\n\n\n \n\nProject 4\n\n\nSome description text.\n\n\n \n\nProject 5\n\n\nAnother fun project summary.\n\n\n\n\n\nStraight HTML\n\n\nuses portfolio-container and card-img but no style on link.\n\n\n \n\nProject 1\n\n\nthis is the thing\n\n\n \n\nProject 2\n\n\nthis is the only thing\n\n\n \n\nProject 3\n\n\nthis is the other thing\n\n\n \n\nProject 1\n\n\nShort description here.\n\n\n \n\nProject 4\n\n\nthis is the other thing again"
  },
  {
    "objectID": "portfolio-test.html",
    "href": "portfolio-test.html",
    "title": "Portfolio Test",
    "section": "",
    "text": "Testing for portfolio page. Uses pure HTML and CSS with flexbox styles.\n\n\n\nin quarto, spacing can matter in HTML code\n\n\nswitching back and forth from Source to Visual can mess up spacing / layout!\n\n\n!IMPORTANT: use Source and remove line-spacing with link tags\n\n\nusing style names like ‘card’ that may be used by boostrap or other framework can produce unexpected effects (but these can be overridden in CSS)\n\n\n\n\ncategory 1\n\n\n \n\nProject 1\n\n\nthis is the thing\n\n\n \n\nProject 2\n\n\nthis is the only thing\n\n\n \n\nProject 3\n\n\nthis is the other thing\n\n\n \n\nProject 4\n\n\nthis is thing using card\n\n\n \n\nProject 5\n\n\nthis is thing using card\n\n\n \n\nProject 6\n\n\nthis is the other thing with card\n\n\n\n\nthis category 2\n\n\nwith only two items - if spacing is off, remove blank lines from HTML code in Source view.\n\n\n \n\nProject 1\n\n\nthis is the thing\n\n\n \n\nProject 2\n\n\nthis is the only thing\n\n\n\n\nSmaller cards more cols\n\n\nSmaller size automatically fills up space avail.:\n\n\n \n\nProject 1\n\n\nShort description here.\n\n\n \n\nProject 2\n\n\nAnother brief detail.\n\n\n \n\nProject 3\n\n\nSomething cool about this one.\n\n\n \n\nProject 4\n\n\nSome description text.\n\n\n \n\nProject 5\n\n\nAnother fun project summary.\n\n\n\n\n\nUnstyled card\n\n\nuses portfolio-container and card-img but no style on link.\n\n\n \n\nProject 1\n\n\nthis is the thing\n\n\n \n\nProject 2\n\n\nthis is the only thing\n\n\n \n\nProject 3\n\n\nthis is the other thing\n\n\n \n\nProject 1\n\n\nShort description here.\n\n\n \n\nProject 4\n\n\nthis is the other thing again\n\n\n\n\nUnstyled with diff container\n\n\nuses portfolio-container-alt: no wrap, adjusts card size to fill space. also justify-content: center but not sure if that does anything.\n\n\n \n\nProject 1\n\n\nthis is the thing\n\n\n \n\nProject 2\n\n\nthis is the only thing\n\n\n \n\nProject 3\n\n\nthis is the other thing\n\n\n \n\nProject 4\n\n\nthis is the other thing"
  },
  {
    "objectID": "portfolio-test-bu.html",
    "href": "portfolio-test-bu.html",
    "title": "Portfolio Test",
    "section": "",
    "text": "Testing for portfolio page. Uses pure HTML and CSS with flexbox styles.\n\n\n\nin quarto, spacing can matter in HTML code\n\n\nswitching back and forth from Source to Visual can mees things up!\n\n\nusing style names like ‘card’ that may be used by boostrap or other framework can produce unexpected effects (but these can be overridden in CSS)\n\n\n\n\ncategory 1\n\n\n \n\nProject 1\n\n\nthis is the thing\n\n\n \n\nProject 2\n\n\nthis is the only thing\n\n\n \n\nProject 3\n\n\nthis is the other thing\n\n\n \n\nProject 4\n\n\nthis is thing using card\n\n\n \n\nProject 5\n\n\nthis is thing using card2\n\n\n \n\nProject 6\n\n\nthis is the other thing with car\n\n\n\n\nthis category 2\n\n\n \n\nProject 1\n\n\nthis is the thing\n\n\n \n\nProject 2\n\n\nthis is the only thing\n\n\n \n\nProject 3\n\n\nthis is the other thing\n\n\n\n\nSmaller cards more cols\n\n\nSmaller size automatically fills up space avail.:\n\n\n \n\nProject 1\n\n\nShort description here.\n\n\n \n\nProject 2\n\n\nAnother brief detail.\n\n\n \n\nProject 3\n\n\nSomething cool about this one.\n\n\n \n\nProject 4\n\n\nSome description text.\n\n\n \n\nProject 5\n\n\nAnother fun project summary.\n\n\n\n\n\nStraight HTML\n\n\nuses portfolio-container and card-img but no style on link.\n\n\n \n\nProject 1\n\n\nthis is the thing\n\n\n \n\nProject 2\n\n\nthis is the only thing\n\n\n \n\nProject 3\n\n\nthis is the other thing\n\n\n \n\nProject 1\n\n\nShort description here.\n\n\n \n\nProject 4\n\n\nthis is the other thing again"
  }
]